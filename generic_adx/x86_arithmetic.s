// Code generated by command: go run main.go -output ./generic_adx -opt D -arch ADX. DO NOT EDIT.

#include "textflag.h"

// func cpy2(dst *[2]uint64, src *[2]uint64)
TEXT ·cpy2(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	RET

// func eq2(a *[2]uint64, b *[2]uint64) bool
TEXT ·eq2(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp2(a *[2]uint64, b *[2]uint64) int8
TEXT ·cmp2(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64)
TEXT ·add2(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R8
	SUBQ (SI), R8
	MOVQ DX, R9
	SBBQ 8(SI), R9
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R8, CX
	MOVQ    CX, (DI)
	CMOVQCC R9, DX
	MOVQ    DX, 8(DI)
	RET

// func addn2(a *[2]uint64, b *[2]uint64) uint64
TEXT ·addn2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double2(c *[2]uint64, a *[2]uint64, p *[2]uint64)
TEXT ·double2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R8
	SUBQ (SI), R8
	MOVQ DX, R9
	SBBQ 8(SI), R9
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R8, CX
	MOVQ    CX, (DI)
	CMOVQCC R9, DX
	MOVQ    DX, 8(DI)
	RET

// func sub2(c *[2]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64)
TEXT ·sub2(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R8
	CMOVQCC AX, R8
	MOVQ    8(SI), R9
	CMOVQCC AX, R9

	// |
	MOVQ c+0(FP), DI
	ADDQ R8, CX
	MOVQ CX, (DI)
	ADCQ R9, DX
	MOVQ DX, 8(DI)
	RET

// func subn2(a *[2]uint64, b *[2]uint64) uint64
TEXT ·subn2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg2(c *[2]uint64, a *[2]uint64, p *[2]uint64)
TEXT ·_neg2(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	RET

// func mul_two_2(a *[2]uint64)
TEXT ·mul_two_2(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RET

// func div_two_2(a *[2]uint64)
TEXT ·div_two_2(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul2(c *[4]uint64, a *[2]uint64, b *[2]uint64, p *[2]uint64, inp uint64)
TEXT ·mul2(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8
	ADCQ  $0x00, R9

	// |
	// |
	MOVQ 8(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9
	ADOXQ BX, R9
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R8, BX

	// | 

/* reduction 				*/

	MOVQ R9, AX
	SUBQ (R15), AX
	MOVQ SI, DX
	SBBQ 8(R15), DX
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R9
	MOVQ    R9, (BX)
	CMOVQCC DX, SI
	MOVQ    SI, 8(BX)
	RET

	// | 

/* end 				*/


// func cpy3(dst *[3]uint64, src *[3]uint64)
TEXT ·cpy3(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	RET

// func eq3(a *[3]uint64, b *[3]uint64) bool
TEXT ·eq3(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp3(a *[3]uint64, b *[3]uint64) int8
TEXT ·cmp3(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64)
TEXT ·add3(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R9
	SUBQ (SI), R9
	MOVQ DX, R10
	SBBQ 8(SI), R10
	MOVQ R8, R11
	SBBQ 16(SI), R11
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R9, CX
	MOVQ    CX, (DI)
	CMOVQCC R10, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R11, R8
	MOVQ    R8, 16(DI)
	RET

// func addn3(a *[3]uint64, b *[3]uint64) uint64
TEXT ·addn3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double3(c *[3]uint64, a *[3]uint64, p *[3]uint64)
TEXT ·double3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R9
	SUBQ (SI), R9
	MOVQ DX, R10
	SBBQ 8(SI), R10
	MOVQ R8, R11
	SBBQ 16(SI), R11
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R9, CX
	MOVQ    CX, (DI)
	CMOVQCC R10, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R11, R8
	MOVQ    R8, 16(DI)
	RET

// func sub3(c *[3]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64)
TEXT ·sub3(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R9
	CMOVQCC AX, R9
	MOVQ    8(SI), R10
	CMOVQCC AX, R10
	MOVQ    16(SI), R11
	CMOVQCC AX, R11

	// |
	MOVQ c+0(FP), DI
	ADDQ R9, CX
	MOVQ CX, (DI)
	ADCQ R10, DX
	MOVQ DX, 8(DI)
	ADCQ R11, R8
	MOVQ R8, 16(DI)
	RET

// func subn3(a *[3]uint64, b *[3]uint64) uint64
TEXT ·subn3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg3(c *[3]uint64, a *[3]uint64, p *[3]uint64)
TEXT ·_neg3(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	RET

// func mul_two_3(a *[3]uint64)
TEXT ·mul_two_3(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RET

// func div_two_3(a *[3]uint64)
TEXT ·div_two_3(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul3(c *[6]uint64, a *[3]uint64, b *[3]uint64, p *[3]uint64, inp uint64)
TEXT ·mul3(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9
	ADCQ  $0x00, R10

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R11, R11

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADOXQ R11, R11
	ADCXQ BX, R11

	// |
	// |
	MOVQ 16(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10
	ADOXQ BX, R10
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11
	ADOXQ BX, R11
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R9, BX

	// | 

/* reduction 				*/

	MOVQ R10, AX
	SUBQ (R15), AX
	MOVQ R11, DX
	SBBQ 8(R15), DX
	MOVQ SI, DI
	SBBQ 16(R15), DI
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R10
	MOVQ    R10, (BX)
	CMOVQCC DX, R11
	MOVQ    R11, 8(BX)
	CMOVQCC DI, SI
	MOVQ    SI, 16(BX)
	RET

	// | 

/* end 				*/


// func cpy4(dst *[4]uint64, src *[4]uint64)
TEXT ·cpy4(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	RET

// func eq4(a *[4]uint64, b *[4]uint64) bool
TEXT ·eq4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp4(a *[4]uint64, b *[4]uint64) int8
TEXT ·cmp4(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·add4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func addn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·addn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·double4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R10
	SUBQ (SI), R10
	MOVQ DX, R11
	SBBQ 8(SI), R11
	MOVQ R8, R12
	SBBQ 16(SI), R12
	MOVQ R9, R13
	SBBQ 24(SI), R13
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R10, CX
	MOVQ    CX, (DI)
	CMOVQCC R11, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R12, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R13, R9
	MOVQ    R9, 24(DI)
	RET

// func sub4(c *[4]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64)
TEXT ·sub4(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R10
	CMOVQCC AX, R10
	MOVQ    8(SI), R11
	CMOVQCC AX, R11
	MOVQ    16(SI), R12
	CMOVQCC AX, R12
	MOVQ    24(SI), R13
	CMOVQCC AX, R13

	// |
	MOVQ c+0(FP), DI
	ADDQ R10, CX
	MOVQ CX, (DI)
	ADCQ R11, DX
	MOVQ DX, 8(DI)
	ADCQ R12, R8
	MOVQ R8, 16(DI)
	ADCQ R13, R9
	MOVQ R9, 24(DI)
	RET

// func subn4(a *[4]uint64, b *[4]uint64) uint64
TEXT ·subn4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg4(c *[4]uint64, a *[4]uint64, p *[4]uint64)
TEXT ·_neg4(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	RET

// func mul_two_4(a *[4]uint64)
TEXT ·mul_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RET

// func div_two_4(a *[4]uint64)
TEXT ·div_two_4(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul4(c *[8]uint64, a *[4]uint64, b *[4]uint64, p *[4]uint64, inp uint64)
TEXT ·mul4(SB), NOSPLIT, $0-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 24(DI), AX, R11
	ADCXQ AX, R10
	ADCQ  $0x00, R11

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R12, R12

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ R12, R12
	ADCXQ BX, R12

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 24(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ p+24(FP), R15

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11
	ADOXQ BX, R11
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12
	ADOXQ BX, R12
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13
	ADOXQ BX, R13
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R9, R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (R15), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 8(R15), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 16(R15), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 24(R15), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, SI
	ADOXQ BX, SI
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R10, BX

	// | 

/* reduction 				*/

	MOVQ R11, AX
	SUBQ (R15), AX
	MOVQ R12, DX
	SBBQ 8(R15), DX
	MOVQ R13, DI
	SBBQ 16(R15), DI
	MOVQ SI, R8
	SBBQ 24(R15), R8
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R11
	MOVQ    R11, (BX)
	CMOVQCC DX, R12
	MOVQ    R12, 8(BX)
	CMOVQCC DI, R13
	MOVQ    R13, 16(BX)
	CMOVQCC R8, SI
	MOVQ    SI, 24(BX)
	RET

	// | 

/* end 				*/


// func cpy5(dst *[5]uint64, src *[5]uint64)
TEXT ·cpy5(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	RET

// func eq5(a *[5]uint64, b *[5]uint64) bool
TEXT ·eq5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp5(a *[5]uint64, b *[5]uint64) int8
TEXT ·cmp5(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·add5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func addn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·addn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·double5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R11
	SUBQ (SI), R11
	MOVQ DX, R12
	SBBQ 8(SI), R12
	MOVQ R8, R13
	SBBQ 16(SI), R13
	MOVQ R9, R14
	SBBQ 24(SI), R14
	MOVQ R10, R15
	SBBQ 32(SI), R15
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R11, CX
	MOVQ    CX, (DI)
	CMOVQCC R12, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R13, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R14, R9
	MOVQ    R9, 24(DI)
	CMOVQCC R15, R10
	MOVQ    R10, 32(DI)
	RET

// func sub5(c *[5]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64)
TEXT ·sub5(SB), NOSPLIT, $0-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R11
	CMOVQCC AX, R11
	MOVQ    8(SI), R12
	CMOVQCC AX, R12
	MOVQ    16(SI), R13
	CMOVQCC AX, R13
	MOVQ    24(SI), R14
	CMOVQCC AX, R14
	MOVQ    32(SI), R15
	CMOVQCC AX, R15

	// |
	MOVQ c+0(FP), DI
	ADDQ R11, CX
	MOVQ CX, (DI)
	ADCQ R12, DX
	MOVQ DX, 8(DI)
	ADCQ R13, R8
	MOVQ R8, 16(DI)
	ADCQ R14, R9
	MOVQ R9, 24(DI)
	ADCQ R15, R10
	MOVQ R10, 32(DI)
	RET

// func subn5(a *[5]uint64, b *[5]uint64) uint64
TEXT ·subn5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg5(c *[5]uint64, a *[5]uint64, p *[5]uint64)
TEXT ·_neg5(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	RET

// func mul_two_5(a *[5]uint64)
TEXT ·mul_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RET

// func div_two_5(a *[5]uint64)
TEXT ·div_two_5(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul5(c *[10]uint64, a *[5]uint64, b *[5]uint64, p *[5]uint64, inp uint64)
TEXT ·mul5(SB), NOSPLIT, $8-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, R8
	MOVQ  AX, CX

	// |
	MULXQ 8(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 16(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 24(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 32(DI), AX, R12
	ADCXQ AX, R11
	ADCQ  $0x00, R12

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 32(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ SI, (SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  BX, BX
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, CX
	ADCXQ DI, R8

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12
	ADOXQ BX, R12
	ADCXQ BX, BX
	XORQ  CX, CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R8
	ADCXQ DI, R9

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13
	ADOXQ BX, R13
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R8, R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R9
	ADCXQ DI, R10

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14
	ADOXQ BX, R14
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R9, R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R10
	ADCXQ DI, R11

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R14
	ADCXQ DI, R15
	ADOXQ BX, R15
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	XORQ  R10, R10
	MOVQ  (SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, DI

	// |
	MULXQ (SI), AX, DI
	ADOXQ AX, R11
	ADCXQ DI, R12

	// |
	MULXQ 8(SI), AX, DI
	ADOXQ AX, R12
	ADCXQ DI, R13

	// |
	MULXQ 16(SI), AX, DI
	ADOXQ AX, R13
	ADCXQ DI, R14

	// |
	MULXQ 24(SI), AX, DI
	ADOXQ AX, R14
	ADCXQ DI, R15

	// |
	MULXQ 32(SI), AX, DI
	ADOXQ AX, R15
	ADCXQ DI, R10
	ADOXQ BX, R10
	MOVQ  $0x00, BX
	ADCXQ BX, BX
	ADOXQ R11, BX

	// | 

/* reduction 				*/

	MOVQ R12, AX
	SUBQ (SI), AX
	MOVQ R13, DX
	SBBQ 8(SI), DX
	MOVQ R14, DI
	SBBQ 16(SI), DI
	MOVQ R15, R8
	SBBQ 24(SI), R8
	MOVQ R10, R9
	SBBQ 32(SI), R9
	SBBQ $0x00, BX

	// |
	MOVQ    c+0(FP), BX
	CMOVQCC AX, R12
	MOVQ    R12, (BX)
	CMOVQCC DX, R13
	MOVQ    R13, 8(BX)
	CMOVQCC DI, R14
	MOVQ    R14, 16(BX)
	CMOVQCC R8, R15
	MOVQ    R15, 24(BX)
	CMOVQCC R9, R10
	MOVQ    R10, 32(BX)
	RET

	// | 

/* end 				*/


// func cpy6(dst *[6]uint64, src *[6]uint64)
TEXT ·cpy6(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	RET

// func eq6(a *[6]uint64, b *[6]uint64) bool
TEXT ·eq6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp6(a *[6]uint64, b *[6]uint64) int8
TEXT ·cmp6(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·add6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func addn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·addn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·double6(SB), NOSPLIT, $16-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R12
	SUBQ (SI), R12
	MOVQ DX, R13
	SBBQ 8(SI), R13
	MOVQ R8, R14
	SBBQ 16(SI), R14
	MOVQ R9, R15
	SBBQ 24(SI), R15
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 8(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R12, CX
	MOVQ    CX, (DI)
	CMOVQCC R13, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R14, R8
	MOVQ    R8, 16(DI)
	CMOVQCC R15, R9
	MOVQ    R9, 24(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 8(SP), R11
	MOVQ    R11, 40(DI)
	RET

// func sub6(c *[6]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64)
TEXT ·sub6(SB), NOSPLIT, $16-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R12
	CMOVQCC AX, R12
	MOVQ    8(SI), R13
	CMOVQCC AX, R13
	MOVQ    16(SI), R14
	CMOVQCC AX, R14
	MOVQ    24(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 32(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 8(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R12, CX
	MOVQ CX, (DI)
	ADCQ R13, DX
	MOVQ DX, 8(DI)
	ADCQ R14, R8
	MOVQ R8, 16(DI)
	ADCQ R15, R9
	MOVQ R9, 24(DI)
	ADCQ (SP), R10
	MOVQ R10, 32(DI)
	ADCQ 8(SP), R11
	MOVQ R11, 40(DI)
	RET

// func subn6(a *[6]uint64, b *[6]uint64) uint64
TEXT ·subn6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg6(c *[6]uint64, a *[6]uint64, p *[6]uint64)
TEXT ·_neg6(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	RET

// func mul_two_6(a *[6]uint64)
TEXT ·mul_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RET

// func div_two_6(a *[6]uint64)
TEXT ·div_two_6(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul6(c *[12]uint64, a *[6]uint64, b *[6]uint64, p *[6]uint64, inp uint64)
TEXT ·mul6(SB), NOSPLIT, $24-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11
	ADCQ  $0x00, R12

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R13, R13

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADOXQ R13, R13
	ADCXQ BX, R13

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 32(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 40(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ CX, 8(SP)
	MOVQ SI, 16(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  CX, CX
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, DI
	ADCXQ R15, BX

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, BX
	ADCXQ R15, R8

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12
	ADOXQ CX, R12
	ADCXQ CX, CX
	XORQ  DI, DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, BX
	ADCXQ R15, R8

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13
	ADOXQ CX, R13
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  BX, BX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14
	ADOXQ CX, R14
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R8, R8
	MOVQ  (SP), R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8
	ADOXQ CX, R8
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R9, R9
	MOVQ  8(SP), R9

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R10
	ADCXQ R15, R11

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9
	ADOXQ CX, R9
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	XORQ  R10, R10
	MOVQ  16(SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, R15

	// |
	MULXQ (SI), AX, R15
	ADOXQ AX, R11
	ADCXQ R15, R12

	// |
	MULXQ 8(SI), AX, R15
	ADOXQ AX, R12
	ADCXQ R15, R13

	// |
	MULXQ 16(SI), AX, R15
	ADOXQ AX, R13
	ADCXQ R15, R14

	// |
	MULXQ 24(SI), AX, R15
	ADOXQ AX, R14
	ADCXQ R15, R8

	// |
	MULXQ 32(SI), AX, R15
	ADOXQ AX, R8
	ADCXQ R15, R9

	// |
	MULXQ 40(SI), AX, R15
	ADOXQ AX, R9
	ADCXQ R15, R10
	ADOXQ CX, R10
	MOVQ  $0x00, CX
	ADCXQ CX, CX
	ADOXQ R11, CX

	// | 

/* reduction 				*/

	MOVQ R12, DX
	SUBQ (SI), DX
	MOVQ R13, BX
	SBBQ 8(SI), BX
	MOVQ R14, DI
	SBBQ 16(SI), DI
	MOVQ R8, R11
	SBBQ 24(SI), R11
	MOVQ R9, R15
	SBBQ 32(SI), R15
	MOVQ R10, AX
	SBBQ 40(SI), AX
	MOVQ AX, (SP)
	SBBQ $0x00, CX

	// |
	MOVQ    c+0(FP), CX
	CMOVQCC DX, R12
	MOVQ    R12, (CX)
	CMOVQCC BX, R13
	MOVQ    R13, 8(CX)
	CMOVQCC DI, R14
	MOVQ    R14, 16(CX)
	CMOVQCC R11, R8
	MOVQ    R8, 24(CX)
	CMOVQCC R15, R9
	MOVQ    R9, 32(CX)
	CMOVQCC (SP), R10
	MOVQ    R10, 40(CX)
	RET

	// | 

/* end 				*/


// func cpy7(dst *[7]uint64, src *[7]uint64)
TEXT ·cpy7(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	RET

// func eq7(a *[7]uint64, b *[7]uint64) bool
TEXT ·eq7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp7(a *[7]uint64, b *[7]uint64) int8
TEXT ·cmp7(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·add7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func addn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·addn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·double7(SB), NOSPLIT, $32-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R13
	SUBQ (SI), R13
	MOVQ DX, R14
	SBBQ 8(SI), R14
	MOVQ R8, R15
	SBBQ 16(SI), R15
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, (SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 24(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R13, CX
	MOVQ    CX, (DI)
	CMOVQCC R14, DX
	MOVQ    DX, 8(DI)
	CMOVQCC R15, R8
	MOVQ    R8, 16(DI)
	CMOVQCC (SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 24(SP), R12
	MOVQ    R12, 48(DI)
	RET

// func sub7(c *[7]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64)
TEXT ·sub7(SB), NOSPLIT, $32-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R13
	CMOVQCC AX, R13
	MOVQ    8(SI), R14
	CMOVQCC AX, R14
	MOVQ    16(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 24(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 24(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R13, CX
	MOVQ CX, (DI)
	ADCQ R14, DX
	MOVQ DX, 8(DI)
	ADCQ R15, R8
	MOVQ R8, 16(DI)
	ADCQ (SP), R9
	MOVQ R9, 24(DI)
	ADCQ 8(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 16(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 24(SP), R12
	MOVQ R12, 48(DI)
	RET

// func subn7(a *[7]uint64, b *[7]uint64) uint64
TEXT ·subn7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg7(c *[7]uint64, a *[7]uint64, p *[7]uint64)
TEXT ·_neg7(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	RET

// func mul_two_7(a *[7]uint64)
TEXT ·mul_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RET

// func div_two_7(a *[7]uint64)
TEXT ·div_two_7(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul7(c *[14]uint64, a *[7]uint64, b *[7]uint64, p *[7]uint64, inp uint64)
TEXT ·mul7(SB), NOSPLIT, $40-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11

	// |
	MULXQ 48(DI), AX, R13
	ADCXQ AX, R12
	ADCQ  $0x00, R13

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R14, R14

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R13
	ADOXQ R14, R14
	ADCXQ BX, R14

	// |
	// |
	MOVQ 16(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9
	MOVQ  R8, 16(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 24(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10
	MOVQ  R9, 24(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 32(SI), DX
	XORQ R8, R8

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ R8, R8
	ADCXQ BX, R8

	// |
	// |
	MOVQ 40(SI), DX
	XORQ R9, R9

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R8
	ADOXQ R9, R9
	ADCXQ BX, R9

	// |
	// |
	MOVQ 48(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ 16(SP), R15
	MOVQ CX, 8(SP)
	MOVQ 24(SP), CX
	MOVQ R8, 16(SP)
	MOVQ R9, 24(SP)
	MOVQ SI, 32(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  R9, R9
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, DI
	ADCXQ R8, BX

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13
	ADOXQ R9, R13
	ADCXQ R9, R9
	XORQ  DI, DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14
	ADOXQ R9, R14
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  BX, BX
	MOVQ  (SP), BX

	// |
	// |
	MOVQ  R15, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX
	ADOXQ R9, BX
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R15, R15
	MOVQ  8(SP), R15

	// |
	// |
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15
	ADOXQ R9, R15
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  CX, CX
	MOVQ  16(SP), CX

	// |
	// |
	MOVQ  R10, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX
	ADOXQ R9, CX
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R10, R10
	MOVQ  24(SP), R10

	// |
	// |
	MOVQ  R11, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R11
	ADCXQ R8, R12

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10
	ADOXQ R9, R10
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	XORQ  R11, R11
	MOVQ  32(SP), R11

	// |
	// |
	MOVQ  R12, DX
	MULXQ inp+32(FP), DX, R8

	// |
	MULXQ (SI), AX, R8
	ADOXQ AX, R12
	ADCXQ R8, R13

	// |
	MULXQ 8(SI), AX, R8
	ADOXQ AX, R13
	ADCXQ R8, R14

	// |
	MULXQ 16(SI), AX, R8
	ADOXQ AX, R14
	ADCXQ R8, BX

	// |
	MULXQ 24(SI), AX, R8
	ADOXQ AX, BX
	ADCXQ R8, R15

	// |
	MULXQ 32(SI), AX, R8
	ADOXQ AX, R15
	ADCXQ R8, CX

	// |
	MULXQ 40(SI), AX, R8
	ADOXQ AX, CX
	ADCXQ R8, R10

	// |
	MULXQ 48(SI), AX, R8
	ADOXQ AX, R10
	ADCXQ R8, R11
	ADOXQ R9, R11
	MOVQ  $0x00, R9
	ADCXQ R9, R9
	ADOXQ R12, R9

	// | 

/* reduction 				*/

	MOVQ R13, DX
	SUBQ (SI), DX
	MOVQ R14, DI
	SBBQ 8(SI), DI
	MOVQ BX, R8
	SBBQ 16(SI), R8
	MOVQ R15, R12
	SBBQ 24(SI), R12
	MOVQ CX, AX
	SBBQ 32(SI), AX
	MOVQ AX, (SP)
	MOVQ R10, AX
	SBBQ 40(SI), AX
	MOVQ AX, 8(SP)
	MOVQ R11, AX
	SBBQ 48(SI), AX
	MOVQ AX, 16(SP)
	SBBQ $0x00, R9

	// |
	MOVQ    c+0(FP), R9
	CMOVQCC DX, R13
	MOVQ    R13, (R9)
	CMOVQCC DI, R14
	MOVQ    R14, 8(R9)
	CMOVQCC R8, BX
	MOVQ    BX, 16(R9)
	CMOVQCC R12, R15
	MOVQ    R15, 24(R9)
	CMOVQCC (SP), CX
	MOVQ    CX, 32(R9)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 40(R9)
	CMOVQCC 16(SP), R11
	MOVQ    R11, 48(R9)
	RET

	// | 

/* end 				*/


// func cpy8(dst *[8]uint64, src *[8]uint64)
TEXT ·cpy8(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	RET

// func eq8(a *[8]uint64, b *[8]uint64) bool
TEXT ·eq8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp8(a *[8]uint64, b *[8]uint64) int8
TEXT ·cmp8(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·add8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func addn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·addn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·double8(SB), NOSPLIT, $48-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R14
	SUBQ (SI), R14
	MOVQ DX, R15
	SBBQ 8(SI), R15
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, (SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 40(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R14, CX
	MOVQ    CX, (DI)
	CMOVQCC R15, DX
	MOVQ    DX, 8(DI)
	CMOVQCC (SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 8(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 24(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 40(SP), R13
	MOVQ    R13, 56(DI)
	RET

// func sub8(c *[8]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64)
TEXT ·sub8(SB), NOSPLIT, $48-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R14
	CMOVQCC AX, R14
	MOVQ    8(SI), R15
	CMOVQCC AX, R15
	CMOVQCS 16(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 40(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R14, CX
	MOVQ CX, (DI)
	ADCQ R15, DX
	MOVQ DX, 8(DI)
	ADCQ (SP), R8
	MOVQ R8, 16(DI)
	ADCQ 8(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 16(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 24(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 32(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 40(SP), R13
	MOVQ R13, 56(DI)
	RET

// func subn8(a *[8]uint64, b *[8]uint64) uint64
TEXT ·subn8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg8(c *[8]uint64, a *[8]uint64, p *[8]uint64)
TEXT ·_neg8(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	RET

// func mul_two_8(a *[8]uint64)
TEXT ·mul_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RET

// func div_two_8(a *[8]uint64)
TEXT ·div_two_8(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul8(c *[16]uint64, a *[8]uint64, b *[8]uint64, p *[8]uint64, inp uint64)
TEXT ·mul8(SB), NOSPLIT, $56-40
	// | 

/* inputs 				*/

	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	// |
	MOVQ (SI), DX

	// |
	MULXQ (DI), AX, CX
	MOVQ  AX, (SP)

	// |
	MULXQ 8(DI), AX, R8
	ADCXQ AX, CX

	// |
	MULXQ 16(DI), AX, R9
	ADCXQ AX, R8

	// |
	MULXQ 24(DI), AX, R10
	ADCXQ AX, R9

	// |
	MULXQ 32(DI), AX, R11
	ADCXQ AX, R10

	// |
	MULXQ 40(DI), AX, R12
	ADCXQ AX, R11

	// |
	MULXQ 48(DI), AX, R13
	ADCXQ AX, R12

	// |
	MULXQ 56(DI), AX, R14
	ADCXQ AX, R13
	ADCQ  $0x00, R14

	// |
	// |
	MOVQ 8(SI), DX
	XORQ R15, R15

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8
	MOVQ  CX, 8(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R14
	ADOXQ R15, R15
	ADCXQ BX, R15

	// |
	// |
	MOVQ 16(SI), DX
	XORQ CX, CX

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9
	MOVQ  R8, 16(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R15
	ADOXQ CX, CX
	ADCXQ BX, CX

	// |
	// |
	MOVQ 24(SI), DX
	XORQ R8, R8

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10
	MOVQ  R9, 24(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, CX
	ADOXQ R8, R8
	ADCXQ BX, R8

	// |
	// |
	MOVQ 32(SI), DX
	XORQ R9, R9

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11
	MOVQ  R10, 32(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R8
	ADOXQ R9, R9
	ADCXQ BX, R9

	// |
	// |
	MOVQ 40(SI), DX
	XORQ R10, R10

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R11
	ADCXQ BX, R12
	MOVQ  R11, 40(SP)

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R9
	ADOXQ R10, R10
	ADCXQ BX, R10

	// |
	// |
	MOVQ 48(SI), DX
	XORQ R11, R11

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R12
	ADCXQ BX, R13

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R10
	ADOXQ R11, R11
	ADCXQ BX, R11

	// |
	// |
	MOVQ 56(SI), DX
	XORQ SI, SI

	// |
	MULXQ (DI), AX, BX
	ADOXQ AX, R13
	ADCXQ BX, R14

	// |
	MULXQ 8(DI), AX, BX
	ADOXQ AX, R14
	ADCXQ BX, R15

	// |
	MULXQ 16(DI), AX, BX
	ADOXQ AX, R15
	ADCXQ BX, CX

	// |
	MULXQ 24(DI), AX, BX
	ADOXQ AX, CX
	ADCXQ BX, R8

	// |
	MULXQ 32(DI), AX, BX
	ADOXQ AX, R8
	ADCXQ BX, R9

	// |
	MULXQ 40(DI), AX, BX
	ADOXQ AX, R9
	ADCXQ BX, R10

	// |
	MULXQ 48(DI), AX, BX
	ADOXQ AX, R10
	ADCXQ BX, R11

	// |
	MULXQ 56(DI), AX, BX
	ADOXQ AX, R11
	ADOXQ BX, SI
	ADCQ  $0x00, SI

	// |
	MOVQ (SP), DI
	MOVQ 8(SP), BX
	MOVQ R15, (SP)
	MOVQ 16(SP), R15
	MOVQ CX, 8(SP)
	MOVQ 24(SP), CX
	MOVQ R8, 16(SP)
	MOVQ 32(SP), R8
	MOVQ R9, 24(SP)
	MOVQ 40(SP), R9
	MOVQ R10, 32(SP)
	MOVQ R11, 40(SP)
	MOVQ SI, 48(SP)
	MOVQ p+24(FP), SI

	// |
	// |
	XORQ  R11, R11
	MOVQ  DI, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14
	ADOXQ R11, R14
	ADCXQ R11, R11
	XORQ  DI, DI
	MOVQ  (SP), DI

	// |
	// |
	MOVQ  BX, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI
	ADOXQ R11, DI
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  BX, BX
	MOVQ  8(SP), BX

	// |
	// |
	MOVQ  R15, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX
	ADOXQ R11, BX
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R15, R15
	MOVQ  16(SP), R15

	// |
	// |
	MOVQ  CX, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15
	ADOXQ R11, R15
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  CX, CX
	MOVQ  24(SP), CX

	// |
	// |
	MOVQ  R8, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX
	ADOXQ R11, CX
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R8, R8
	MOVQ  32(SP), R8

	// |
	// |
	MOVQ  R9, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8
	ADOXQ R11, R8
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R9, R9
	MOVQ  40(SP), R9

	// |
	// |
	MOVQ  R12, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R12
	ADCXQ R10, R13

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9
	ADOXQ R11, R9
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	XORQ  R12, R12
	MOVQ  48(SP), R12

	// |
	// |
	MOVQ  R13, DX
	MULXQ inp+32(FP), DX, R10

	// |
	MULXQ (SI), AX, R10
	ADOXQ AX, R13
	ADCXQ R10, R14

	// |
	MULXQ 8(SI), AX, R10
	ADOXQ AX, R14
	ADCXQ R10, DI

	// |
	MULXQ 16(SI), AX, R10
	ADOXQ AX, DI
	ADCXQ R10, BX

	// |
	MULXQ 24(SI), AX, R10
	ADOXQ AX, BX
	ADCXQ R10, R15

	// |
	MULXQ 32(SI), AX, R10
	ADOXQ AX, R15
	ADCXQ R10, CX

	// |
	MULXQ 40(SI), AX, R10
	ADOXQ AX, CX
	ADCXQ R10, R8

	// |
	MULXQ 48(SI), AX, R10
	ADOXQ AX, R8
	ADCXQ R10, R9

	// |
	MULXQ 56(SI), AX, R10
	ADOXQ AX, R9
	ADCXQ R10, R12
	ADOXQ R11, R12
	MOVQ  $0x00, R11
	ADCXQ R11, R11
	ADOXQ R13, R11

	// | 

/* reduction 				*/

	MOVQ R14, DX
	SUBQ (SI), DX
	MOVQ DI, R10
	SBBQ 8(SI), R10
	MOVQ BX, R13
	SBBQ 16(SI), R13
	MOVQ R15, AX
	SBBQ 24(SI), AX
	MOVQ AX, (SP)
	MOVQ CX, AX
	SBBQ 32(SI), AX
	MOVQ AX, 8(SP)
	MOVQ R8, AX
	SBBQ 40(SI), AX
	MOVQ AX, 16(SP)
	MOVQ R9, AX
	SBBQ 48(SI), AX
	MOVQ AX, 24(SP)
	MOVQ R12, AX
	SBBQ 56(SI), AX
	MOVQ AX, 32(SP)
	SBBQ $0x00, R11

	// |
	MOVQ    c+0(FP), R11
	CMOVQCC DX, R14
	MOVQ    R14, (R11)
	CMOVQCC R10, DI
	MOVQ    DI, 8(R11)
	CMOVQCC R13, BX
	MOVQ    BX, 16(R11)
	CMOVQCC (SP), R15
	MOVQ    R15, 24(R11)
	CMOVQCC 8(SP), CX
	MOVQ    CX, 32(R11)
	CMOVQCC 16(SP), R8
	MOVQ    R8, 40(R11)
	CMOVQCC 24(SP), R9
	MOVQ    R9, 48(R11)
	CMOVQCC 32(SP), R12
	MOVQ    R12, 56(R11)
	RET

	// | 

/* end 				*/



// func cpy9(dst *[9]uint64, src *[9]uint64)
TEXT ·cpy9(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	RET

// func eq9(a *[9]uint64, b *[9]uint64) bool
TEXT ·eq9(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp9(a *[9]uint64, b *[9]uint64) int8
TEXT ·cmp9(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add9(c *[9]uint64, a *[9]uint64, b *[9]uint64, p *[9]uint64)
TEXT ·add9(SB), NOSPLIT, $64-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, R15
	SUBQ (SI), R15
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, (SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 56(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R15, CX
	MOVQ    CX, (DI)
	CMOVQCC (SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 8(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 16(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 24(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 32(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 40(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 48(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 56(SP), R14
	MOVQ    R14, 64(DI)
	RET

// func addn9(a *[9]uint64, b *[9]uint64) uint64
TEXT ·addn9(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double9(c *[9]uint64, a *[9]uint64, p *[9]uint64)
TEXT ·double9(SB), NOSPLIT, $64-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, R15
	SUBQ (SI), R15
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, (SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 56(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC R15, CX
	MOVQ    CX, (DI)
	CMOVQCC (SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 8(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 16(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 24(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 32(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 40(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 48(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 56(SP), R14
	MOVQ    R14, 64(DI)
	RET

// func sub9(c *[9]uint64, a *[9]uint64, b *[9]uint64, p *[9]uint64)
TEXT ·sub9(SB), NOSPLIT, $64-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14

	// |
	MOVQ    p+24(FP), SI
	MOVQ    (SI), R15
	CMOVQCC AX, R15
	CMOVQCS 8(SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 40(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 56(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ R15, CX
	MOVQ CX, (DI)
	ADCQ (SP), DX
	MOVQ DX, 8(DI)
	ADCQ 8(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 16(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 24(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 32(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 40(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 48(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 56(SP), R14
	MOVQ R14, 64(DI)
	RET

// func subn9(a *[9]uint64, b *[9]uint64) uint64
TEXT ·subn9(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg9(c *[9]uint64, a *[9]uint64, p *[9]uint64)
TEXT ·_neg9(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	RET

// func mul_two_9(a *[9]uint64)
TEXT ·mul_two_9(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RET

// func div_two_9(a *[9]uint64)
TEXT ·div_two_9(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy10(dst *[10]uint64, src *[10]uint64)
TEXT ·cpy10(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	MOVQ 72(SI), R8
	MOVQ R8, 72(DI)
	RET

// func eq10(a *[10]uint64, b *[10]uint64) bool
TEXT ·eq10(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp10(a *[10]uint64, b *[10]uint64) int8
TEXT ·cmp10(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JB   gt
	JA   lt
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add10(c *[10]uint64, a *[10]uint64, b *[10]uint64, p *[10]uint64)
TEXT ·add10(SB), NOSPLIT, $80-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, (SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 72(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC (SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 8(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 16(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 24(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 32(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 40(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 48(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 56(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 64(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 72(SP), R15
	MOVQ    R15, 72(DI)
	RET

// func addn10(a *[10]uint64, b *[10]uint64) uint64
TEXT ·addn10(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double10(c *[10]uint64, a *[10]uint64, p *[10]uint64)
TEXT ·double10(SB), NOSPLIT, $80-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	MOVQ 72(DI), R15
	ADCQ R15, R15
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, (SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 8(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 72(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC (SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 8(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 16(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 24(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 32(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 40(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 48(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 56(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 64(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 72(SP), R15
	MOVQ    R15, 72(DI)
	RET

// func sub10(c *[10]uint64, a *[10]uint64, b *[10]uint64, p *[10]uint64)
TEXT ·sub10(SB), NOSPLIT, $80-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15

	// |
	MOVQ    p+24(FP), SI
	CMOVQCS (SI), AX
	MOVQ    AX, (SP)
	CMOVQCS 8(SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 40(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 56(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 64(SP)
	CMOVQCS 72(SI), AX
	MOVQ    AX, 72(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ (SP), CX
	MOVQ CX, (DI)
	ADCQ 8(SP), DX
	MOVQ DX, 8(DI)
	ADCQ 16(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 24(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 32(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 40(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 48(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 56(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 64(SP), R14
	MOVQ R14, 64(DI)
	ADCQ 72(SP), R15
	MOVQ R15, 72(DI)
	RET

// func subn10(a *[10]uint64, b *[10]uint64) uint64
TEXT ·subn10(SB), NOSPLIT, $0-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg10(c *[10]uint64, a *[10]uint64, p *[10]uint64)
TEXT ·_neg10(SB), NOSPLIT, $0-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14
	MOVQ 72(SI), R15
	SBBQ 72(DI), R15

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	RET

// func mul_two_10(a *[10]uint64)
TEXT ·mul_two_10(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RCLQ $0x01, 72(DI)
	RET

// func div_two_10(a *[10]uint64)
TEXT ·div_two_10(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 72(DI)
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy11(dst *[11]uint64, src *[11]uint64)
TEXT ·cpy11(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	MOVQ 72(SI), R8
	MOVQ R8, 72(DI)
	MOVQ 80(SI), R8
	MOVQ R8, 80(DI)
	RET

// func eq11(a *[11]uint64, b *[11]uint64) bool
TEXT ·eq11(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JNE  ret
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp11(a *[11]uint64, b *[11]uint64) int8
TEXT ·cmp11(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JB   gt
	JA   lt
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JB   gt
	JA   lt
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add11(c *[11]uint64, a *[11]uint64, b *[11]uint64, p *[11]uint64)
TEXT ·add11(SB), NOSPLIT, $96-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 8(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 80(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 88(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 8(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 16(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 24(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 32(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 40(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 48(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 56(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 64(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 72(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 80(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 88(SP), BX
	MOVQ    BX, 80(DI)
	RET

// func addn11(a *[11]uint64, b *[11]uint64) uint64
TEXT ·addn11(SB), NOSPLIT, $8-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double11(c *[11]uint64, a *[11]uint64, p *[11]uint64)
TEXT ·double11(SB), NOSPLIT, $96-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	MOVQ 72(DI), R15
	ADCQ R15, R15
	MOVQ 80(DI), BX
	ADCQ BX, BX
	MOVQ BX, (SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 8(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 16(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 80(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 88(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 8(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 16(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 24(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 32(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 40(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 48(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 56(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 64(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 72(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 80(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 88(SP), BX
	MOVQ    BX, 80(DI)
	RET

// func sub11(c *[11]uint64, a *[11]uint64, b *[11]uint64, p *[11]uint64)
TEXT ·sub11(SB), NOSPLIT, $96-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)

	// |
	MOVQ    p+24(FP), SI
	CMOVQCS (SI), AX
	MOVQ    AX, 8(SP)
	CMOVQCS 8(SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 40(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 56(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 64(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 72(SP)
	CMOVQCS 72(SI), AX
	MOVQ    AX, 80(SP)
	CMOVQCS 80(SI), AX
	MOVQ    AX, 88(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ 8(SP), CX
	MOVQ CX, (DI)
	ADCQ 16(SP), DX
	MOVQ DX, 8(DI)
	ADCQ 24(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 32(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 40(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 48(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 56(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 64(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 72(SP), R14
	MOVQ R14, 64(DI)
	ADCQ 80(SP), R15
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	ADCQ 88(SP), BX
	MOVQ BX, 80(DI)
	RET

// func subn11(a *[11]uint64, b *[11]uint64) uint64
TEXT ·subn11(SB), NOSPLIT, $8-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg11(c *[11]uint64, a *[11]uint64, p *[11]uint64)
TEXT ·_neg11(SB), NOSPLIT, $8-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14
	MOVQ 72(SI), R15
	SBBQ 72(DI), R15
	MOVQ 80(SI), BX
	SBBQ 80(DI), BX
	MOVQ BX, (SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	RET

// func mul_two_11(a *[11]uint64)
TEXT ·mul_two_11(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RCLQ $0x01, 72(DI)
	RCLQ $0x01, 80(DI)
	RET

// func div_two_11(a *[11]uint64)
TEXT ·div_two_11(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 80(DI)
	RCRQ $0x01, 72(DI)
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET


// func cpy12(dst *[12]uint64, src *[12]uint64)
TEXT ·cpy12(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	MOVQ 72(SI), R8
	MOVQ R8, 72(DI)
	MOVQ 80(SI), R8
	MOVQ R8, 80(DI)
	MOVQ 88(SI), R8
	MOVQ R8, 88(DI)
	RET

// func eq12(a *[12]uint64, b *[12]uint64) bool
TEXT ·eq12(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JNE  ret
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JNE  ret
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp12(a *[12]uint64, b *[12]uint64) int8
TEXT ·cmp12(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JB   gt
	JA   lt
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JB   gt
	JA   lt
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JB   gt
	JA   lt
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add12(c *[12]uint64, a *[12]uint64, b *[12]uint64, p *[12]uint64)
TEXT ·add12(SB), NOSPLIT, $112-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 16(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 88(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 96(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 104(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 16(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 24(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 32(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 40(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 48(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 56(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 64(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 72(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 80(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 88(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 96(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 104(SP), BX
	MOVQ    BX, 88(DI)
	RET

// func addn12(a *[12]uint64, b *[12]uint64) uint64
TEXT ·addn12(SB), NOSPLIT, $16-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double12(c *[12]uint64, a *[12]uint64, p *[12]uint64)
TEXT ·double12(SB), NOSPLIT, $112-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	MOVQ 72(DI), R15
	ADCQ R15, R15
	MOVQ 80(DI), BX
	ADCQ BX, BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ BX, BX
	MOVQ BX, 8(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 16(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 24(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 88(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 96(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 104(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 16(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 24(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 32(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 40(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 48(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 56(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 64(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 72(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 80(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 88(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 96(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 104(SP), BX
	MOVQ    BX, 88(DI)
	RET

// func sub12(c *[12]uint64, a *[12]uint64, b *[12]uint64, p *[12]uint64)
TEXT ·sub12(SB), NOSPLIT, $112-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)

	// |
	MOVQ    p+24(FP), SI
	CMOVQCS (SI), AX
	MOVQ    AX, 16(SP)
	CMOVQCS 8(SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 40(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 56(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 64(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 72(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 80(SP)
	CMOVQCS 72(SI), AX
	MOVQ    AX, 88(SP)
	CMOVQCS 80(SI), AX
	MOVQ    AX, 96(SP)
	CMOVQCS 88(SI), AX
	MOVQ    AX, 104(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ 16(SP), CX
	MOVQ CX, (DI)
	ADCQ 24(SP), DX
	MOVQ DX, 8(DI)
	ADCQ 32(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 40(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 48(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 56(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 64(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 72(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 80(SP), R14
	MOVQ R14, 64(DI)
	ADCQ 88(SP), R15
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	ADCQ 96(SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	ADCQ 104(SP), BX
	MOVQ BX, 88(DI)
	RET

// func subn12(a *[12]uint64, b *[12]uint64) uint64
TEXT ·subn12(SB), NOSPLIT, $16-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg12(c *[12]uint64, a *[12]uint64, p *[12]uint64)
TEXT ·_neg12(SB), NOSPLIT, $16-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14
	MOVQ 72(SI), R15
	SBBQ 72(DI), R15
	MOVQ 80(SI), BX
	SBBQ 80(DI), BX
	MOVQ BX, (SP)
	MOVQ 88(SI), BX
	SBBQ 88(DI), BX
	MOVQ BX, 8(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	RET

// func mul_two_12(a *[12]uint64)
TEXT ·mul_two_12(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RCLQ $0x01, 72(DI)
	RCLQ $0x01, 80(DI)
	RCLQ $0x01, 88(DI)
	RET

// func div_two_12(a *[12]uint64)
TEXT ·div_two_12(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 88(DI)
	RCRQ $0x01, 80(DI)
	RCRQ $0x01, 72(DI)
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET


// func cpy13(dst *[13]uint64, src *[13]uint64)
TEXT ·cpy13(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	MOVQ 72(SI), R8
	MOVQ R8, 72(DI)
	MOVQ 80(SI), R8
	MOVQ R8, 80(DI)
	MOVQ 88(SI), R8
	MOVQ R8, 88(DI)
	MOVQ 96(SI), R8
	MOVQ R8, 96(DI)
	RET

// func eq13(a *[13]uint64, b *[13]uint64) bool
TEXT ·eq13(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JNE  ret
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JNE  ret
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JNE  ret
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp13(a *[13]uint64, b *[13]uint64) int8
TEXT ·cmp13(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JB   gt
	JA   lt
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JB   gt
	JA   lt
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JB   gt
	JA   lt
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JB   gt
	JA   lt
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add13(c *[13]uint64, a *[13]uint64, b *[13]uint64, p *[13]uint64)
TEXT ·add13(SB), NOSPLIT, $128-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 24(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 96(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 104(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 112(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 120(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 24(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 32(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 40(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 48(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 56(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 64(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 72(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 80(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 88(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 96(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 104(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 112(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 120(SP), BX
	MOVQ    BX, 96(DI)
	RET

// func addn13(a *[13]uint64, b *[13]uint64) uint64
TEXT ·addn13(SB), NOSPLIT, $24-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double13(c *[13]uint64, a *[13]uint64, p *[13]uint64)
TEXT ·double13(SB), NOSPLIT, $128-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	MOVQ 72(DI), R15
	ADCQ R15, R15
	MOVQ 80(DI), BX
	ADCQ BX, BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ BX, BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ BX, BX
	MOVQ BX, 16(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 24(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 32(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 96(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 104(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 112(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 120(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 24(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 32(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 40(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 48(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 56(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 64(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 72(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 80(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 88(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 96(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 104(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 112(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 120(SP), BX
	MOVQ    BX, 96(DI)
	RET

// func sub13(c *[13]uint64, a *[13]uint64, b *[13]uint64, p *[13]uint64)
TEXT ·sub13(SB), NOSPLIT, $128-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)

	// |
	MOVQ    p+24(FP), SI
	CMOVQCS (SI), AX
	MOVQ    AX, 24(SP)
	CMOVQCS 8(SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 40(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 56(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 64(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 72(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 80(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 88(SP)
	CMOVQCS 72(SI), AX
	MOVQ    AX, 96(SP)
	CMOVQCS 80(SI), AX
	MOVQ    AX, 104(SP)
	CMOVQCS 88(SI), AX
	MOVQ    AX, 112(SP)
	CMOVQCS 96(SI), AX
	MOVQ    AX, 120(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ 24(SP), CX
	MOVQ CX, (DI)
	ADCQ 32(SP), DX
	MOVQ DX, 8(DI)
	ADCQ 40(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 48(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 56(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 64(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 72(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 80(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 88(SP), R14
	MOVQ R14, 64(DI)
	ADCQ 96(SP), R15
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	ADCQ 104(SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	ADCQ 112(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	ADCQ 120(SP), BX
	MOVQ BX, 96(DI)
	RET

// func subn13(a *[13]uint64, b *[13]uint64) uint64
TEXT ·subn13(SB), NOSPLIT, $24-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg13(c *[13]uint64, a *[13]uint64, p *[13]uint64)
TEXT ·_neg13(SB), NOSPLIT, $24-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14
	MOVQ 72(SI), R15
	SBBQ 72(DI), R15
	MOVQ 80(SI), BX
	SBBQ 80(DI), BX
	MOVQ BX, (SP)
	MOVQ 88(SI), BX
	SBBQ 88(DI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(SI), BX
	SBBQ 96(DI), BX
	MOVQ BX, 16(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	RET

// func mul_two_13(a *[13]uint64)
TEXT ·mul_two_13(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RCLQ $0x01, 72(DI)
	RCLQ $0x01, 80(DI)
	RCLQ $0x01, 88(DI)
	RCLQ $0x01, 96(DI)
	RET

// func div_two_13(a *[13]uint64)
TEXT ·div_two_13(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 96(DI)
	RCRQ $0x01, 88(DI)
	RCRQ $0x01, 80(DI)
	RCRQ $0x01, 72(DI)
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET


// func cpy14(dst *[14]uint64, src *[14]uint64)
TEXT ·cpy14(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	MOVQ 72(SI), R8
	MOVQ R8, 72(DI)
	MOVQ 80(SI), R8
	MOVQ R8, 80(DI)
	MOVQ 88(SI), R8
	MOVQ R8, 88(DI)
	MOVQ 96(SI), R8
	MOVQ R8, 96(DI)
	MOVQ 104(SI), R8
	MOVQ R8, 104(DI)
	RET

// func eq14(a *[14]uint64, b *[14]uint64) bool
TEXT ·eq14(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JNE  ret
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JNE  ret
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JNE  ret
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JNE  ret
	MOVQ 104(DI), R8
	CMPQ 104(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp14(a *[14]uint64, b *[14]uint64) int8
TEXT ·cmp14(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 104(DI), R8
	CMPQ 104(SI), R8
	JB   gt
	JA   lt
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JB   gt
	JA   lt
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JB   gt
	JA   lt
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JB   gt
	JA   lt
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JB   gt
	JA   lt
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add14(c *[14]uint64, a *[14]uint64, b *[14]uint64, p *[14]uint64)
TEXT ·add14(SB), NOSPLIT, $144-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ 104(SI), BX
	MOVQ BX, 24(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 32(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 96(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 104(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 112(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 120(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 128(SP)
	MOVQ 24(SP), BX
	SBBQ 104(SI), BX
	MOVQ BX, 136(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 32(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 40(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 48(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 56(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 64(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 72(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 80(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 88(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 96(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 104(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 112(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 120(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 128(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    24(SP), BX
	CMOVQCC 136(SP), BX
	MOVQ    BX, 104(DI)
	RET

// func addn14(a *[14]uint64, b *[14]uint64) uint64
TEXT ·addn14(SB), NOSPLIT, $32-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ 104(SI), BX
	MOVQ BX, 24(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double14(c *[14]uint64, a *[14]uint64, p *[14]uint64)
TEXT ·double14(SB), NOSPLIT, $144-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	MOVQ 72(DI), R15
	ADCQ R15, R15
	MOVQ 80(DI), BX
	ADCQ BX, BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ BX, BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ BX, BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ BX, BX
	MOVQ BX, 24(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 32(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 40(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 96(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 104(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 112(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 120(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 128(SP)
	MOVQ 24(SP), BX
	SBBQ 104(SI), BX
	MOVQ BX, 136(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 32(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 40(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 48(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 56(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 64(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 72(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 80(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 88(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 96(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 104(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 112(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 120(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 128(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    24(SP), BX
	CMOVQCC 136(SP), BX
	MOVQ    BX, 104(DI)
	RET

// func sub14(c *[14]uint64, a *[14]uint64, b *[14]uint64, p *[14]uint64)
TEXT ·sub14(SB), NOSPLIT, $144-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	SBBQ 104(SI), BX
	MOVQ BX, 24(SP)

	// |
	MOVQ    p+24(FP), SI
	CMOVQCS (SI), AX
	MOVQ    AX, 32(SP)
	CMOVQCS 8(SI), AX
	MOVQ    AX, 40(SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 56(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 64(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 72(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 80(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 88(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 96(SP)
	CMOVQCS 72(SI), AX
	MOVQ    AX, 104(SP)
	CMOVQCS 80(SI), AX
	MOVQ    AX, 112(SP)
	CMOVQCS 88(SI), AX
	MOVQ    AX, 120(SP)
	CMOVQCS 96(SI), AX
	MOVQ    AX, 128(SP)
	CMOVQCS 104(SI), AX
	MOVQ    AX, 136(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ 32(SP), CX
	MOVQ CX, (DI)
	ADCQ 40(SP), DX
	MOVQ DX, 8(DI)
	ADCQ 48(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 56(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 64(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 72(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 80(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 88(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 96(SP), R14
	MOVQ R14, 64(DI)
	ADCQ 104(SP), R15
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	ADCQ 112(SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	ADCQ 120(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	ADCQ 128(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	ADCQ 136(SP), BX
	MOVQ BX, 104(DI)
	RET

// func subn14(a *[14]uint64, b *[14]uint64) uint64
TEXT ·subn14(SB), NOSPLIT, $32-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	SBBQ 104(SI), BX
	MOVQ BX, 24(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg14(c *[14]uint64, a *[14]uint64, p *[14]uint64)
TEXT ·_neg14(SB), NOSPLIT, $32-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14
	MOVQ 72(SI), R15
	SBBQ 72(DI), R15
	MOVQ 80(SI), BX
	SBBQ 80(DI), BX
	MOVQ BX, (SP)
	MOVQ 88(SI), BX
	SBBQ 88(DI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(SI), BX
	SBBQ 96(DI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(SI), BX
	SBBQ 104(DI), BX
	MOVQ BX, 24(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	RET

// func mul_two_14(a *[14]uint64)
TEXT ·mul_two_14(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RCLQ $0x01, 72(DI)
	RCLQ $0x01, 80(DI)
	RCLQ $0x01, 88(DI)
	RCLQ $0x01, 96(DI)
	RCLQ $0x01, 104(DI)
	RET

// func div_two_14(a *[14]uint64)
TEXT ·div_two_14(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 104(DI)
	RCRQ $0x01, 96(DI)
	RCRQ $0x01, 88(DI)
	RCRQ $0x01, 80(DI)
	RCRQ $0x01, 72(DI)
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy15(dst *[15]uint64, src *[15]uint64)
TEXT ·cpy15(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	MOVQ 72(SI), R8
	MOVQ R8, 72(DI)
	MOVQ 80(SI), R8
	MOVQ R8, 80(DI)
	MOVQ 88(SI), R8
	MOVQ R8, 88(DI)
	MOVQ 96(SI), R8
	MOVQ R8, 96(DI)
	MOVQ 104(SI), R8
	MOVQ R8, 104(DI)
	MOVQ 112(SI), R8
	MOVQ R8, 112(DI)
	RET

// func eq15(a *[15]uint64, b *[15]uint64) bool
TEXT ·eq15(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JNE  ret
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JNE  ret
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JNE  ret
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JNE  ret
	MOVQ 104(DI), R8
	CMPQ 104(SI), R8
	JNE  ret
	MOVQ 112(DI), R8
	CMPQ 112(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp15(a *[15]uint64, b *[15]uint64) int8
TEXT ·cmp15(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 112(DI), R8
	CMPQ 112(SI), R8
	JB   gt
	JA   lt
	MOVQ 104(DI), R8
	CMPQ 104(SI), R8
	JB   gt
	JA   lt
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JB   gt
	JA   lt
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JB   gt
	JA   lt
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JB   gt
	JA   lt
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JB   gt
	JA   lt
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add15(c *[15]uint64, a *[15]uint64, b *[15]uint64, p *[15]uint64)
TEXT ·add15(SB), NOSPLIT, $160-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	ADCQ 112(SI), BX
	MOVQ BX, 32(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 40(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 96(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 104(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 112(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 120(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 128(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 136(SP)
	MOVQ 24(SP), BX
	SBBQ 104(SI), BX
	MOVQ BX, 144(SP)
	MOVQ 32(SP), BX
	SBBQ 112(SI), BX
	MOVQ BX, 152(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 40(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 48(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 56(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 64(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 72(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 80(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 88(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 96(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 104(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 112(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 120(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 128(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 136(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    24(SP), BX
	CMOVQCC 144(SP), BX
	MOVQ    BX, 104(DI)
	MOVQ    32(SP), BX
	CMOVQCC 152(SP), BX
	MOVQ    BX, 112(DI)
	RET

// func addn15(a *[15]uint64, b *[15]uint64) uint64
TEXT ·addn15(SB), NOSPLIT, $40-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	ADCQ 112(SI), BX
	MOVQ BX, 32(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 112(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double15(c *[15]uint64, a *[15]uint64, p *[15]uint64)
TEXT ·double15(SB), NOSPLIT, $160-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	MOVQ 72(DI), R15
	ADCQ R15, R15
	MOVQ 80(DI), BX
	ADCQ BX, BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ BX, BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ BX, BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ BX, BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	ADCQ BX, BX
	MOVQ BX, 32(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 40(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 48(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 96(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 104(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 112(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 120(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 128(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 136(SP)
	MOVQ 24(SP), BX
	SBBQ 104(SI), BX
	MOVQ BX, 144(SP)
	MOVQ 32(SP), BX
	SBBQ 112(SI), BX
	MOVQ BX, 152(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 40(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 48(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 56(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 64(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 72(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 80(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 88(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 96(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 104(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 112(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 120(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 128(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 136(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    24(SP), BX
	CMOVQCC 144(SP), BX
	MOVQ    BX, 104(DI)
	MOVQ    32(SP), BX
	CMOVQCC 152(SP), BX
	MOVQ    BX, 112(DI)
	RET

// func sub15(c *[15]uint64, a *[15]uint64, b *[15]uint64, p *[15]uint64)
TEXT ·sub15(SB), NOSPLIT, $160-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	SBBQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	SBBQ 112(SI), BX
	MOVQ BX, 32(SP)

	// |
	MOVQ    p+24(FP), SI
	CMOVQCS (SI), AX
	MOVQ    AX, 40(SP)
	CMOVQCS 8(SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 56(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 64(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 72(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 80(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 88(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 96(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 104(SP)
	CMOVQCS 72(SI), AX
	MOVQ    AX, 112(SP)
	CMOVQCS 80(SI), AX
	MOVQ    AX, 120(SP)
	CMOVQCS 88(SI), AX
	MOVQ    AX, 128(SP)
	CMOVQCS 96(SI), AX
	MOVQ    AX, 136(SP)
	CMOVQCS 104(SI), AX
	MOVQ    AX, 144(SP)
	CMOVQCS 112(SI), AX
	MOVQ    AX, 152(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ 40(SP), CX
	MOVQ CX, (DI)
	ADCQ 48(SP), DX
	MOVQ DX, 8(DI)
	ADCQ 56(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 64(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 72(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 80(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 88(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 96(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 104(SP), R14
	MOVQ R14, 64(DI)
	ADCQ 112(SP), R15
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	ADCQ 120(SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	ADCQ 128(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	ADCQ 136(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	ADCQ 144(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	ADCQ 152(SP), BX
	MOVQ BX, 112(DI)
	RET

// func subn15(a *[15]uint64, b *[15]uint64) uint64
TEXT ·subn15(SB), NOSPLIT, $40-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	SBBQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	SBBQ 112(SI), BX
	MOVQ BX, 32(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 112(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg15(c *[15]uint64, a *[15]uint64, p *[15]uint64)
TEXT ·_neg15(SB), NOSPLIT, $40-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14
	MOVQ 72(SI), R15
	SBBQ 72(DI), R15
	MOVQ 80(SI), BX
	SBBQ 80(DI), BX
	MOVQ BX, (SP)
	MOVQ 88(SI), BX
	SBBQ 88(DI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(SI), BX
	SBBQ 96(DI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(SI), BX
	SBBQ 104(DI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(SI), BX
	SBBQ 112(DI), BX
	MOVQ BX, 32(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 112(DI)
	RET

// func mul_two_15(a *[15]uint64)
TEXT ·mul_two_15(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RCLQ $0x01, 72(DI)
	RCLQ $0x01, 80(DI)
	RCLQ $0x01, 88(DI)
	RCLQ $0x01, 96(DI)
	RCLQ $0x01, 104(DI)
	RCLQ $0x01, 112(DI)
	RET

// func div_two_15(a *[15]uint64)
TEXT ·div_two_15(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 112(DI)
	RCRQ $0x01, 104(DI)
	RCRQ $0x01, 96(DI)
	RCRQ $0x01, 88(DI)
	RCRQ $0x01, 80(DI)
	RCRQ $0x01, 72(DI)
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func cpy16(dst *[16]uint64, src *[16]uint64)
TEXT ·cpy16(SB), NOSPLIT, $0-16
	MOVQ dst+0(FP), DI
	MOVQ src+8(FP), SI
	MOVQ (SI), R8
	MOVQ R8, (DI)
	MOVQ 8(SI), R8
	MOVQ R8, 8(DI)
	MOVQ 16(SI), R8
	MOVQ R8, 16(DI)
	MOVQ 24(SI), R8
	MOVQ R8, 24(DI)
	MOVQ 32(SI), R8
	MOVQ R8, 32(DI)
	MOVQ 40(SI), R8
	MOVQ R8, 40(DI)
	MOVQ 48(SI), R8
	MOVQ R8, 48(DI)
	MOVQ 56(SI), R8
	MOVQ R8, 56(DI)
	MOVQ 64(SI), R8
	MOVQ R8, 64(DI)
	MOVQ 72(SI), R8
	MOVQ R8, 72(DI)
	MOVQ 80(SI), R8
	MOVQ R8, 80(DI)
	MOVQ 88(SI), R8
	MOVQ R8, 88(DI)
	MOVQ 96(SI), R8
	MOVQ R8, 96(DI)
	MOVQ 104(SI), R8
	MOVQ R8, 104(DI)
	MOVQ 112(SI), R8
	MOVQ R8, 112(DI)
	MOVQ 120(SI), R8
	MOVQ R8, 120(DI)
	RET

// func eq16(a *[16]uint64, b *[16]uint64) bool
TEXT ·eq16(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVB $0x00, ret+16(FP)
	MOVQ (DI), R8
	CMPQ (SI), R8
	JNE  ret
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JNE  ret
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JNE  ret
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JNE  ret
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JNE  ret
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JNE  ret
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JNE  ret
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JNE  ret
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JNE  ret
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JNE  ret
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JNE  ret
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JNE  ret
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JNE  ret
	MOVQ 104(DI), R8
	CMPQ 104(SI), R8
	JNE  ret
	MOVQ 112(DI), R8
	CMPQ 112(SI), R8
	JNE  ret
	MOVQ 120(DI), R8
	CMPQ 120(SI), R8
	JNE  ret
	MOVB $0x01, ret+16(FP)

ret:
	RET

// func cmp16(a *[16]uint64, b *[16]uint64) int8
TEXT ·cmp16(SB), NOSPLIT, $0-17
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	MOVQ 120(DI), R8
	CMPQ 120(SI), R8
	JB   gt
	JA   lt
	MOVQ 112(DI), R8
	CMPQ 112(SI), R8
	JB   gt
	JA   lt
	MOVQ 104(DI), R8
	CMPQ 104(SI), R8
	JB   gt
	JA   lt
	MOVQ 96(DI), R8
	CMPQ 96(SI), R8
	JB   gt
	JA   lt
	MOVQ 88(DI), R8
	CMPQ 88(SI), R8
	JB   gt
	JA   lt
	MOVQ 80(DI), R8
	CMPQ 80(SI), R8
	JB   gt
	JA   lt
	MOVQ 72(DI), R8
	CMPQ 72(SI), R8
	JB   gt
	JA   lt
	MOVQ 64(DI), R8
	CMPQ 64(SI), R8
	JB   gt
	JA   lt
	MOVQ 56(DI), R8
	CMPQ 56(SI), R8
	JB   gt
	JA   lt
	MOVQ 48(DI), R8
	CMPQ 48(SI), R8
	JB   gt
	JA   lt
	MOVQ 40(DI), R8
	CMPQ 40(SI), R8
	JB   gt
	JA   lt
	MOVQ 32(DI), R8
	CMPQ 32(SI), R8
	JB   gt
	JA   lt
	MOVQ 24(DI), R8
	CMPQ 24(SI), R8
	JB   gt
	JA   lt
	MOVQ 16(DI), R8
	CMPQ 16(SI), R8
	JB   gt
	JA   lt
	MOVQ 8(DI), R8
	CMPQ 8(SI), R8
	JB   gt
	JA   lt
	MOVQ (DI), R8
	CMPQ (SI), R8
	JB   gt
	JA   lt
	MOVB $0x00, ret+16(FP)
	JMP  ret

gt:
	MOVB $0x01, ret+16(FP)
	JMP  ret

lt:
	MOVB $0xff, ret+16(FP)

ret:
	RET

// func add16(c *[16]uint64, a *[16]uint64, b *[16]uint64, p *[16]uint64)
TEXT ·add16(SB), NOSPLIT, $176-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	ADCQ 112(SI), BX
	MOVQ BX, 32(SP)
	MOVQ 120(DI), BX
	ADCQ 120(SI), BX
	MOVQ BX, 40(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+24(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 48(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 96(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 104(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 112(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 120(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 128(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 136(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 144(SP)
	MOVQ 24(SP), BX
	SBBQ 104(SI), BX
	MOVQ BX, 152(SP)
	MOVQ 32(SP), BX
	SBBQ 112(SI), BX
	MOVQ BX, 160(SP)
	MOVQ 40(SP), BX
	SBBQ 120(SI), BX
	MOVQ BX, 168(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 48(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 56(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 64(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 72(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 80(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 88(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 96(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 104(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 112(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 120(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 128(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 136(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 144(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    24(SP), BX
	CMOVQCC 152(SP), BX
	MOVQ    BX, 104(DI)
	MOVQ    32(SP), BX
	CMOVQCC 160(SP), BX
	MOVQ    BX, 112(DI)
	MOVQ    40(SP), BX
	CMOVQCC 168(SP), BX
	MOVQ    BX, 120(DI)
	RET

// func addn16(a *[16]uint64, b *[16]uint64) uint64
TEXT ·addn16(SB), NOSPLIT, $48-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI

	// |
	MOVQ (DI), CX
	ADDQ (SI), CX
	MOVQ 8(DI), DX
	ADCQ 8(SI), DX
	MOVQ 16(DI), R8
	ADCQ 16(SI), R8
	MOVQ 24(DI), R9
	ADCQ 24(SI), R9
	MOVQ 32(DI), R10
	ADCQ 32(SI), R10
	MOVQ 40(DI), R11
	ADCQ 40(SI), R11
	MOVQ 48(DI), R12
	ADCQ 48(SI), R12
	MOVQ 56(DI), R13
	ADCQ 56(SI), R13
	MOVQ 64(DI), R14
	ADCQ 64(SI), R14
	MOVQ 72(DI), R15
	ADCQ 72(SI), R15
	MOVQ 80(DI), BX
	ADCQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	ADCQ 112(SI), BX
	MOVQ BX, 32(SP)
	MOVQ 120(DI), BX
	ADCQ 120(SI), BX
	MOVQ BX, 40(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 112(DI)
	MOVQ 40(SP), BX
	MOVQ BX, 120(DI)
	MOVQ AX, ret+16(FP)
	RET

// func double16(c *[16]uint64, a *[16]uint64, p *[16]uint64)
TEXT ·double16(SB), NOSPLIT, $176-24
	// |
	MOVQ a+8(FP), DI
	XORQ AX, AX
	MOVQ (DI), CX
	ADDQ CX, CX
	MOVQ 8(DI), DX
	ADCQ DX, DX
	MOVQ 16(DI), R8
	ADCQ R8, R8
	MOVQ 24(DI), R9
	ADCQ R9, R9
	MOVQ 32(DI), R10
	ADCQ R10, R10
	MOVQ 40(DI), R11
	ADCQ R11, R11
	MOVQ 48(DI), R12
	ADCQ R12, R12
	MOVQ 56(DI), R13
	ADCQ R13, R13
	MOVQ 64(DI), R14
	ADCQ R14, R14
	MOVQ 72(DI), R15
	ADCQ R15, R15
	MOVQ 80(DI), BX
	ADCQ BX, BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	ADCQ BX, BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	ADCQ BX, BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	ADCQ BX, BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	ADCQ BX, BX
	MOVQ BX, 32(SP)
	MOVQ 120(DI), BX
	ADCQ BX, BX
	MOVQ BX, 40(SP)
	ADCQ $0x00, AX

	// |
	MOVQ p+16(FP), SI
	MOVQ CX, BX
	SUBQ (SI), BX
	MOVQ BX, 48(SP)
	MOVQ DX, BX
	SBBQ 8(SI), BX
	MOVQ BX, 56(SP)
	MOVQ R8, BX
	SBBQ 16(SI), BX
	MOVQ BX, 64(SP)
	MOVQ R9, BX
	SBBQ 24(SI), BX
	MOVQ BX, 72(SP)
	MOVQ R10, BX
	SBBQ 32(SI), BX
	MOVQ BX, 80(SP)
	MOVQ R11, BX
	SBBQ 40(SI), BX
	MOVQ BX, 88(SP)
	MOVQ R12, BX
	SBBQ 48(SI), BX
	MOVQ BX, 96(SP)
	MOVQ R13, BX
	SBBQ 56(SI), BX
	MOVQ BX, 104(SP)
	MOVQ R14, BX
	SBBQ 64(SI), BX
	MOVQ BX, 112(SP)
	MOVQ R15, BX
	SBBQ 72(SI), BX
	MOVQ BX, 120(SP)
	MOVQ (SP), BX
	SBBQ 80(SI), BX
	MOVQ BX, 128(SP)
	MOVQ 8(SP), BX
	SBBQ 88(SI), BX
	MOVQ BX, 136(SP)
	MOVQ 16(SP), BX
	SBBQ 96(SI), BX
	MOVQ BX, 144(SP)
	MOVQ 24(SP), BX
	SBBQ 104(SI), BX
	MOVQ BX, 152(SP)
	MOVQ 32(SP), BX
	SBBQ 112(SI), BX
	MOVQ BX, 160(SP)
	MOVQ 40(SP), BX
	SBBQ 120(SI), BX
	MOVQ BX, 168(SP)
	SBBQ $0x00, AX

	// |
	MOVQ    c+0(FP), DI
	CMOVQCC 48(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC 56(SP), DX
	MOVQ    DX, 8(DI)
	CMOVQCC 64(SP), R8
	MOVQ    R8, 16(DI)
	CMOVQCC 72(SP), R9
	MOVQ    R9, 24(DI)
	CMOVQCC 80(SP), R10
	MOVQ    R10, 32(DI)
	CMOVQCC 88(SP), R11
	MOVQ    R11, 40(DI)
	CMOVQCC 96(SP), R12
	MOVQ    R12, 48(DI)
	CMOVQCC 104(SP), R13
	MOVQ    R13, 56(DI)
	CMOVQCC 112(SP), R14
	MOVQ    R14, 64(DI)
	CMOVQCC 120(SP), R15
	MOVQ    R15, 72(DI)
	MOVQ    (SP), BX
	CMOVQCC 128(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    8(SP), BX
	CMOVQCC 136(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    16(SP), BX
	CMOVQCC 144(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    24(SP), BX
	CMOVQCC 152(SP), BX
	MOVQ    BX, 104(DI)
	MOVQ    32(SP), BX
	CMOVQCC 160(SP), BX
	MOVQ    BX, 112(DI)
	MOVQ    40(SP), BX
	CMOVQCC 168(SP), BX
	MOVQ    BX, 120(DI)
	RET

// func sub16(c *[16]uint64, a *[16]uint64, b *[16]uint64, p *[16]uint64)
TEXT ·sub16(SB), NOSPLIT, $176-32
	// |
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI
	XORQ AX, AX
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	SBBQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	SBBQ 112(SI), BX
	MOVQ BX, 32(SP)
	MOVQ 120(DI), BX
	SBBQ 120(SI), BX
	MOVQ BX, 40(SP)

	// |
	MOVQ    p+24(FP), SI
	CMOVQCS (SI), AX
	MOVQ    AX, 48(SP)
	CMOVQCS 8(SI), AX
	MOVQ    AX, 56(SP)
	CMOVQCS 16(SI), AX
	MOVQ    AX, 64(SP)
	CMOVQCS 24(SI), AX
	MOVQ    AX, 72(SP)
	CMOVQCS 32(SI), AX
	MOVQ    AX, 80(SP)
	CMOVQCS 40(SI), AX
	MOVQ    AX, 88(SP)
	CMOVQCS 48(SI), AX
	MOVQ    AX, 96(SP)
	CMOVQCS 56(SI), AX
	MOVQ    AX, 104(SP)
	CMOVQCS 64(SI), AX
	MOVQ    AX, 112(SP)
	CMOVQCS 72(SI), AX
	MOVQ    AX, 120(SP)
	CMOVQCS 80(SI), AX
	MOVQ    AX, 128(SP)
	CMOVQCS 88(SI), AX
	MOVQ    AX, 136(SP)
	CMOVQCS 96(SI), AX
	MOVQ    AX, 144(SP)
	CMOVQCS 104(SI), AX
	MOVQ    AX, 152(SP)
	CMOVQCS 112(SI), AX
	MOVQ    AX, 160(SP)
	CMOVQCS 120(SI), AX
	MOVQ    AX, 168(SP)

	// |
	MOVQ c+0(FP), DI
	ADDQ 48(SP), CX
	MOVQ CX, (DI)
	ADCQ 56(SP), DX
	MOVQ DX, 8(DI)
	ADCQ 64(SP), R8
	MOVQ R8, 16(DI)
	ADCQ 72(SP), R9
	MOVQ R9, 24(DI)
	ADCQ 80(SP), R10
	MOVQ R10, 32(DI)
	ADCQ 88(SP), R11
	MOVQ R11, 40(DI)
	ADCQ 96(SP), R12
	MOVQ R12, 48(DI)
	ADCQ 104(SP), R13
	MOVQ R13, 56(DI)
	ADCQ 112(SP), R14
	MOVQ R14, 64(DI)
	ADCQ 120(SP), R15
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	ADCQ 128(SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	ADCQ 136(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	ADCQ 144(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	ADCQ 152(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	ADCQ 160(SP), BX
	MOVQ BX, 112(DI)
	MOVQ 40(SP), BX
	ADCQ 168(SP), BX
	MOVQ BX, 120(DI)
	RET

// func subn16(a *[16]uint64, b *[16]uint64) uint64
TEXT ·subn16(SB), NOSPLIT, $48-24
	// |
	MOVQ a+0(FP), DI
	MOVQ b+8(FP), SI
	XORQ AX, AX

	// |
	MOVQ (DI), CX
	SUBQ (SI), CX
	MOVQ 8(DI), DX
	SBBQ 8(SI), DX
	MOVQ 16(DI), R8
	SBBQ 16(SI), R8
	MOVQ 24(DI), R9
	SBBQ 24(SI), R9
	MOVQ 32(DI), R10
	SBBQ 32(SI), R10
	MOVQ 40(DI), R11
	SBBQ 40(SI), R11
	MOVQ 48(DI), R12
	SBBQ 48(SI), R12
	MOVQ 56(DI), R13
	SBBQ 56(SI), R13
	MOVQ 64(DI), R14
	SBBQ 64(SI), R14
	MOVQ 72(DI), R15
	SBBQ 72(SI), R15
	MOVQ 80(DI), BX
	SBBQ 80(SI), BX
	MOVQ BX, (SP)
	MOVQ 88(DI), BX
	SBBQ 88(SI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(DI), BX
	SBBQ 96(SI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(DI), BX
	SBBQ 104(SI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(DI), BX
	SBBQ 112(SI), BX
	MOVQ BX, 32(SP)
	MOVQ 120(DI), BX
	SBBQ 120(SI), BX
	MOVQ BX, 40(SP)
	ADCQ $0x00, AX

	// |
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 112(DI)
	MOVQ 40(SP), BX
	MOVQ BX, 120(DI)
	MOVQ AX, ret+16(FP)
	RET

// func _neg16(c *[16]uint64, a *[16]uint64, p *[16]uint64)
TEXT ·_neg16(SB), NOSPLIT, $48-24
	// |
	MOVQ a+8(FP), DI

	// |
	MOVQ p+16(FP), SI
	MOVQ (SI), CX
	SUBQ (DI), CX
	MOVQ 8(SI), DX
	SBBQ 8(DI), DX
	MOVQ 16(SI), R8
	SBBQ 16(DI), R8
	MOVQ 24(SI), R9
	SBBQ 24(DI), R9
	MOVQ 32(SI), R10
	SBBQ 32(DI), R10
	MOVQ 40(SI), R11
	SBBQ 40(DI), R11
	MOVQ 48(SI), R12
	SBBQ 48(DI), R12
	MOVQ 56(SI), R13
	SBBQ 56(DI), R13
	MOVQ 64(SI), R14
	SBBQ 64(DI), R14
	MOVQ 72(SI), R15
	SBBQ 72(DI), R15
	MOVQ 80(SI), BX
	SBBQ 80(DI), BX
	MOVQ BX, (SP)
	MOVQ 88(SI), BX
	SBBQ 88(DI), BX
	MOVQ BX, 8(SP)
	MOVQ 96(SI), BX
	SBBQ 96(DI), BX
	MOVQ BX, 16(SP)
	MOVQ 104(SI), BX
	SBBQ 104(DI), BX
	MOVQ BX, 24(SP)
	MOVQ 112(SI), BX
	SBBQ 112(DI), BX
	MOVQ BX, 32(SP)
	MOVQ 120(SI), BX
	SBBQ 120(DI), BX
	MOVQ BX, 40(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ CX, (DI)
	MOVQ DX, 8(DI)
	MOVQ R8, 16(DI)
	MOVQ R9, 24(DI)
	MOVQ R10, 32(DI)
	MOVQ R11, 40(DI)
	MOVQ R12, 48(DI)
	MOVQ R13, 56(DI)
	MOVQ R14, 64(DI)
	MOVQ R15, 72(DI)
	MOVQ (SP), BX
	MOVQ BX, 80(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 112(DI)
	MOVQ 40(SP), BX
	MOVQ BX, 120(DI)
	RET

// func mul_two_16(a *[16]uint64)
TEXT ·mul_two_16(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCLQ $0x01, (DI)
	RCLQ $0x01, 8(DI)
	RCLQ $0x01, 16(DI)
	RCLQ $0x01, 24(DI)
	RCLQ $0x01, 32(DI)
	RCLQ $0x01, 40(DI)
	RCLQ $0x01, 48(DI)
	RCLQ $0x01, 56(DI)
	RCLQ $0x01, 64(DI)
	RCLQ $0x01, 72(DI)
	RCLQ $0x01, 80(DI)
	RCLQ $0x01, 88(DI)
	RCLQ $0x01, 96(DI)
	RCLQ $0x01, 104(DI)
	RCLQ $0x01, 112(DI)
	RCLQ $0x01, 120(DI)
	RET

// func div_two_16(a *[16]uint64)
TEXT ·div_two_16(SB), NOSPLIT, $0-8
	MOVQ a+0(FP), DI
	XORQ AX, AX
	RCRQ $0x01, 120(DI)
	RCRQ $0x01, 112(DI)
	RCRQ $0x01, 104(DI)
	RCRQ $0x01, 96(DI)
	RCRQ $0x01, 88(DI)
	RCRQ $0x01, 80(DI)
	RCRQ $0x01, 72(DI)
	RCRQ $0x01, 64(DI)
	RCRQ $0x01, 56(DI)
	RCRQ $0x01, 48(DI)
	RCRQ $0x01, 40(DI)
	RCRQ $0x01, 32(DI)
	RCRQ $0x01, 24(DI)
	RCRQ $0x01, 16(DI)
	RCRQ $0x01, 8(DI)
	RCRQ $0x01, (DI)
	RET

// func mul9(c *[18]uint64, a *[9]uint64, b *[9]uint64, p *[9]uint64, inp uint64)
TEXT ·mul9(SB), $136-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17) @ (56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17) @ (56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b8
	// | (w16, w17) @ (64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 80(SP)
	MOVQ R14, 88(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 88(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, DI
	ADDQ SI, 88(SP)
	ADCQ $0x00, DI

	// | w7 @ 80(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, SI
	ADDQ DI, 80(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	ADDQ DI, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 16(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 88(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, SI
	ADDQ DI, 88(SP)
	ADCQ $0x00, SI

	// | w7 @ 80(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, DI
	ADDQ SI, 80(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ R8
	ADDQ DI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 24(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 88(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, DI
	ADDQ SI, 88(SP)
	ADCQ $0x00, DI

	// | w7 @ 80(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, SI
	ADDQ DI, 80(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w11 @ R9
	ADDQ DI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 32(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 88(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, SI
	ADDQ DI, 88(SP)
	ADCQ $0x00, SI

	// | w7 @ 80(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, DI
	ADDQ SI, 80(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w11 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w12 @ R10
	ADDQ DI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 40(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 88(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, DI
	ADDQ SI, 88(SP)
	ADCQ $0x00, DI

	// | w7 @ 80(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, SI
	ADDQ DI, 80(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w11 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w12 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w13 @ R11
	ADDQ DI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 88(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, SI
	ADDQ DI, 88(SP)
	ADCQ $0x00, SI

	// | w7 @ 80(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, DI
	ADDQ SI, 80(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w11 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w12 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w13 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w14 @ R12
	ADDQ DI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(SP), R13

	// | (u @ CX) = (w6 @ 88(SP)) * inp
	MOVQ 88(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 88(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, DI

	// | w7 @ 80(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, SI
	ADDQ DI, 80(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w11 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w12 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w13 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w14 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w15 @ R13
	ADDQ DI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(SP), BX
	MOVQ BX, 88(SP)

	// | (u @ CX) = (w7 @ 80(SP)) * inp
	MOVQ 80(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 80(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w11 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w12 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w13 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w14 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w15 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w16 @ 88(SP)
	ADDQ DI, R15
	ADCQ R15, 88(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), BX
	MOVQ BX, 80(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w11 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w12 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w13 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w14 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w15 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w16 @ 88(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, DI
	ADDQ SI, 88(SP)
	ADCQ $0x00, DI

	// | w17 @ 80(SP)
	ADDQ DI, R15
	ADCQ R15, 80(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 8(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, (SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 96(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 104(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 112(SP)
	MOVQ 88(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 120(SP)
	MOVQ 80(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 128(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 8(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC (SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 96(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 104(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 112(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    88(SP), BX
	CMOVQCC 120(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    80(SP), BX
	CMOVQCC 128(SP), BX
	MOVQ    BX, 64(DI)
	RET


// func mul10(c *[20]uint64, a *[10]uint64, b *[10]uint64, p *[10]uint64, inp uint64)
TEXT ·mul10(SB), $160-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// | a9 * b0
	// | (w9, w10) @ (8(SP), 16(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a9 * b1
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a9 * b2
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a9 * b3
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a9 * b4
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a9 * b5
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a9 * b6
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a9 * b7
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8 * b8
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b8
	// | (w17, w18, w19) @ (72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b9
	MOVQ 72(SI), CX

	// | a0 * b9
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a1 * b9
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a2 * b9
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a3 * b9
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a4 * b9
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a5 * b9
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a6 * b9
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a7 * b9
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a8 * b9
	// | (w17, w18, w19) @ (72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b9
	// | (w18, w19) @ (80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 96(SP)
	MOVQ R14, 104(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 104(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, DI
	ADDQ SI, 104(SP)
	ADCQ $0x00, DI

	// | w7 @ 96(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, SI
	ADDQ DI, 96(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	ADDQ SI, 16(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 24(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 104(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, SI
	ADDQ DI, 104(SP)
	ADCQ $0x00, SI

	// | w7 @ 96(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, DI
	ADDQ SI, 96(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ R8
	ADDQ SI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 32(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 104(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, DI
	ADDQ SI, 104(SP)
	ADCQ $0x00, DI

	// | w7 @ 96(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, SI
	ADDQ DI, 96(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ R8
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w12 @ R9
	ADDQ SI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 40(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 104(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, SI
	ADDQ DI, 104(SP)
	ADCQ $0x00, SI

	// | w7 @ 96(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, DI
	ADDQ SI, 96(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w12 @ R9
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w13 @ R10
	ADDQ SI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 104(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, DI
	ADDQ SI, 104(SP)
	ADCQ $0x00, DI

	// | w7 @ 96(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, SI
	ADDQ DI, 96(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w12 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w13 @ R10
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w14 @ R11
	ADDQ SI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 104(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, SI
	ADDQ DI, 104(SP)
	ADCQ $0x00, SI

	// | w7 @ 96(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, DI
	ADDQ SI, 96(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w12 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w13 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w14 @ R11
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w15 @ R12
	ADDQ SI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(SP), R13

	// | (u @ CX) = (w6 @ 104(SP)) * inp
	MOVQ 104(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 104(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, DI

	// | w7 @ 96(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, SI
	ADDQ DI, 96(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w12 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w13 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w14 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w15 @ R12
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w16 @ R13
	ADDQ SI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), BX
	MOVQ BX, 104(SP)

	// | (u @ CX) = (w7 @ 96(SP)) * inp
	MOVQ 96(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 96(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w12 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w13 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w14 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w15 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w16 @ R13
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w17 @ 104(SP)
	ADDQ SI, R15
	ADCQ R15, 104(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(SP), BX
	MOVQ BX, 96(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w12 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w13 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w14 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w15 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w16 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w17 @ 104(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, SI
	ADDQ DI, 104(SP)
	ADCQ $0x00, SI

	// | w18 @ 96(SP)
	ADDQ SI, R15
	ADCQ R15, 96(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(SP), BX
	MOVQ BX, (SP)

	// | (u @ CX) = (w9 @ 8(SP)) * inp
	MOVQ 8(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w12 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w13 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w14 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w15 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w16 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w17 @ 104(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, DI
	ADDQ SI, 104(SP)
	ADCQ $0x00, DI

	// | w18 @ 96(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, SI
	ADDQ DI, 96(SP)
	ADCQ $0x00, SI

	// | w19 @ (SP)
	ADDQ SI, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 16(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, 8(SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 112(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 120(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 128(SP)
	MOVQ 104(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 136(SP)
	MOVQ 96(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 144(SP)
	MOVQ (SP), BX
	SBBQ 72(R14), BX
	MOVQ BX, 152(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 16(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC 8(SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 112(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 120(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 128(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    104(SP), BX
	CMOVQCC 136(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    96(SP), BX
	CMOVQCC 144(SP), BX
	MOVQ    BX, 64(DI)
	MOVQ    (SP), BX
	CMOVQCC 152(SP), BX
	MOVQ    BX, 72(DI)
	RET


// func mul11(c *[22]uint64, a *[11]uint64, b *[11]uint64, p *[11]uint64, inp uint64)
TEXT ·mul11(SB), $184-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)
	MOVQ $0x00000000, 96(SP)
	MOVQ $0x00000000, 104(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// | a9 * b0
	// | (w9, w10) @ (8(SP), 16(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)

	// | a10 * b0
	// | (w10, w11) @ (16(SP), 24(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a9 * b1
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a10 * b1
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a9 * b2
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a10 * b2
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a9 * b3
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a10 * b3
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a9 * b4
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a10 * b4
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a9 * b5
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a10 * b5
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a9 * b6
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a10 * b6
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a9 * b7
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a10 * b7
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8 * b8
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b8
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a10 * b8
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// |
	// | b9
	MOVQ 72(SI), CX

	// | a0 * b9
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a1 * b9
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a2 * b9
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a3 * b9
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a4 * b9
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a5 * b9
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a6 * b9
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a7 * b9
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a8 * b9
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a9 * b9
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a10 * b9
	// | (w19, w20, w21) @ (88(SP), 96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)

	// |
	// | b10
	MOVQ 80(SI), CX

	// | a0 * b10
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a1 * b10
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a2 * b10
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a3 * b10
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a4 * b10
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a5 * b10
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a6 * b10
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a7 * b10
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a8 * b10
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a9 * b10
	// | (w19, w20, w21) @ (88(SP), 96(SP), 104(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a10 * b10
	// | (w20, w21) @ (96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 112(SP)
	MOVQ R14, 120(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 120(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, DI
	ADDQ SI, 120(SP)
	ADCQ $0x00, DI

	// | w7 @ 112(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, SI
	ADDQ DI, 112(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	ADDQ DI, 24(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 32(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 120(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, SI
	ADDQ DI, 120(SP)
	ADCQ $0x00, SI

	// | w7 @ 112(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, DI
	ADDQ SI, 112(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ R8
	ADDQ DI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 40(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 120(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, DI
	ADDQ SI, 120(SP)
	ADCQ $0x00, DI

	// | w7 @ 112(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, SI
	ADDQ DI, 112(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ R8
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w13 @ R9
	ADDQ DI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 120(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, SI
	ADDQ DI, 120(SP)
	ADCQ $0x00, SI

	// | w7 @ 112(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, DI
	ADDQ SI, 112(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ R8
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w13 @ R9
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w14 @ R10
	ADDQ DI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 120(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, DI
	ADDQ SI, 120(SP)
	ADCQ $0x00, DI

	// | w7 @ 112(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, SI
	ADDQ DI, 112(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w13 @ R9
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w14 @ R10
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w15 @ R11
	ADDQ DI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 120(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, SI
	ADDQ DI, 120(SP)
	ADCQ $0x00, SI

	// | w7 @ 112(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, DI
	ADDQ SI, 112(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w13 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w14 @ R10
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w15 @ R11
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w16 @ R12
	ADDQ DI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), R13

	// | (u @ CX) = (w6 @ 120(SP)) * inp
	MOVQ 120(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 120(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, DI

	// | w7 @ 112(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, SI
	ADDQ DI, 112(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w13 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w14 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w15 @ R11
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w16 @ R12
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w17 @ R13
	ADDQ DI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(SP), BX
	MOVQ BX, 120(SP)

	// | (u @ CX) = (w7 @ 112(SP)) * inp
	MOVQ 112(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 112(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w13 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w14 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w15 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w16 @ R12
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w17 @ R13
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w18 @ 120(SP)
	ADDQ DI, R15
	ADCQ R15, 120(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(SP), BX
	MOVQ BX, 112(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w13 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w14 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w15 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w16 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w17 @ R13
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w18 @ 120(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, DI
	ADDQ SI, 120(SP)
	ADCQ $0x00, DI

	// | w19 @ 112(SP)
	ADDQ DI, R15
	ADCQ R15, 112(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(SP), BX
	MOVQ BX, (SP)

	// | (u @ CX) = (w9 @ 8(SP)) * inp
	MOVQ 8(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w13 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w14 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w15 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w16 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w17 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w18 @ 120(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, SI
	ADDQ DI, 120(SP)
	ADCQ $0x00, SI

	// | w19 @ 112(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, DI
	ADDQ SI, 112(SP)
	ADCQ $0x00, DI

	// | w20 @ (SP)
	ADDQ DI, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(SP), BX
	MOVQ BX, 8(SP)

	// | (u @ CX) = (w10 @ 16(SP)) * inp
	MOVQ 16(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w13 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w14 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w15 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w16 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w17 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w18 @ 120(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, DI
	ADDQ SI, 120(SP)
	ADCQ $0x00, DI

	// | w19 @ 112(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, SI
	ADDQ DI, 112(SP)
	ADCQ $0x00, SI

	// | w20 @ (SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w21 @ 8(SP)
	ADDQ DI, R15
	ADCQ R15, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 24(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, 16(SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 128(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 136(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 144(SP)
	MOVQ 120(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 152(SP)
	MOVQ 112(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 160(SP)
	MOVQ (SP), BX
	SBBQ 72(R14), BX
	MOVQ BX, 168(SP)
	MOVQ 8(SP), BX
	SBBQ 80(R14), BX
	MOVQ BX, 176(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 24(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC 16(SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 128(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 136(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 144(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    120(SP), BX
	CMOVQCC 152(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    112(SP), BX
	CMOVQCC 160(SP), BX
	MOVQ    BX, 64(DI)
	MOVQ    (SP), BX
	CMOVQCC 168(SP), BX
	MOVQ    BX, 72(DI)
	MOVQ    8(SP), BX
	CMOVQCC 176(SP), BX
	MOVQ    BX, 80(DI)
	RET

// func square11(c *[22]uint64, a *fe704, p *fe704)
TEXT ·square11(SB), $104-24
/* inputs 				*/
	// |
	MOVQ a+8(FP), DI
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	XORQ CX, CX
	XORQ SI, SI
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)
	MOVQ $0x00000000, 96(SP)

	// | a0
	// | w0 @ R9
	MOVQ (DI), R8
	MOVQ R8, AX
	MULQ R8
	MOVQ AX, R9
	MOVQ DX, R10

	// | w1 @ R10
	MOVQ 8(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, R12
	ADDQ AX, R10
	ADCQ DX, R11

	// | w2 @ R11
	MOVQ 16(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, R13
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13

	// | w3 @ R12
	MOVQ 24(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, R14
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14

	// | w4 @ R13
	MOVQ 32(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, R15
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15

	// | w5 @ R14
	MOVQ 40(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, CX

	// | w6 @ R15
	MOVQ 48(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, SI
	ADDQ AX, R15
	ADCQ DX, CX
	ADCQ $0x00, SI

	// | w7 @ CX
	MOVQ 56(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, (SP)
	ADDQ AX, CX
	ADCQ DX, SI
	ADCQ $0x00, (SP)

	// | w8 @ SI
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 8(SP)
	ADDQ AX, SI
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// | w9 @ (SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 16(SP)
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)

	// | w10 @ 8(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 24(SP)
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1
	// | w2 @ R11
	MOVQ 8(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | w3 @ R12
	MOVQ 16(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, R14
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14

	// | w4 @ R13
	MOVQ 24(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, R15
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15

	// | w5 @ R14
	MOVQ 32(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, CX

	// | w6 @ R15
	MOVQ 40(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, SI
	ADDQ AX, R15
	ADCQ DX, CX
	ADCQ $0x00, SI

	// | w7 @ CX
	MOVQ 48(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, (SP)
	ADDQ AX, CX
	ADCQ DX, SI
	ADCQ $0x00, (SP)

	// | w8 @ SI
	MOVQ 56(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 8(SP)
	ADDQ AX, SI
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// | w9 @ (SP)
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 16(SP)
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)

	// | w10 @ 8(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 24(SP)
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// | w11 @ 16(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 32(SP)
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2
	// | w4 @ R13
	MOVQ 16(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, CX

	// | w5 @ R14
	MOVQ 24(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, CX

	// | w6 @ R15
	MOVQ 32(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, SI
	ADDQ AX, R15
	ADCQ DX, CX
	ADCQ $0x00, SI

	// | w7 @ CX
	MOVQ 40(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, (SP)
	ADDQ AX, CX
	ADCQ DX, SI
	ADCQ $0x00, (SP)

	// | w8 @ SI
	MOVQ 48(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 8(SP)
	ADDQ AX, SI
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// | w9 @ (SP)
	MOVQ 56(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 16(SP)
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)

	// | w10 @ 8(SP)
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 24(SP)
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// | w11 @ 16(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 32(SP)
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)

	// | w12 @ 24(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 40(SP)
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3
	// | w6 @ R15
	MOVQ 24(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, R15
	ADCQ DX, CX
	ADCQ $0x00, SI
	ADCQ $0x00, (SP)

	// | w7 @ CX
	MOVQ 32(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, (SP)
	ADDQ AX, CX
	ADCQ DX, SI
	ADCQ $0x00, (SP)

	// | w8 @ SI
	MOVQ 40(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 8(SP)
	ADDQ AX, SI
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)

	// | w9 @ (SP)
	MOVQ 48(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 16(SP)
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)

	// | w10 @ 8(SP)
	MOVQ 56(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 24(SP)
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// | w11 @ 16(SP)
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 32(SP)
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)

	// | w12 @ 24(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 40(SP)
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// | w13 @ 32(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 48(SP)
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4
	// | w8 @ SI
	MOVQ 32(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, SI
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | w9 @ (SP)
	MOVQ 40(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 16(SP)
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)

	// | w10 @ 8(SP)
	MOVQ 48(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 24(SP)
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)

	// | w11 @ 16(SP)
	MOVQ 56(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 32(SP)
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)

	// | w12 @ 24(SP)
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 40(SP)
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// | w13 @ 32(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 48(SP)
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)

	// | w14 @ 40(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 56(SP)
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5
	// | w10 @ 8(SP)
	MOVQ 40(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | w11 @ 16(SP)
	MOVQ 48(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 32(SP)
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)

	// | w12 @ 24(SP)
	MOVQ 56(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 40(SP)
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)

	// | w13 @ 32(SP)
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 48(SP)
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)

	// | w14 @ 40(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 56(SP)
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)

	// | w15 @ 48(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 64(SP)
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6
	// | w12 @ 24(SP)
	MOVQ 48(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | w13 @ 32(SP)
	MOVQ 56(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 48(SP)
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)

	// | w14 @ 40(SP)
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 56(SP)
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)

	// | w15 @ 48(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 64(SP)
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)

	// | w16 @ 56(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 72(SP)
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7
	// | w14 @ 40(SP)
	MOVQ 56(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | w15 @ 48(SP)
	MOVQ 64(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 64(SP)
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)

	// | w16 @ 56(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 72(SP)
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)

	// | w17 @ 64(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 80(SP)
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8
	// | w16 @ 56(SP)
	MOVQ 64(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | w17 @ 64(SP)
	MOVQ 72(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 80(SP)
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)

	// | w18 @ 72(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 88(SP)
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9
	// | w18 @ 72(SP)
	MOVQ 72(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)

	// | w19 @ 80(SP)
	MOVQ 80(DI), AX
	MULQ R8
	ADDQ AX, AX
	ADCQ DX, DX
	ADCQ $0x00, 96(SP)
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a10
	// | w20 @ 88(SP)
	MOVQ 80(DI), R8
	MOVQ R8, AX
	MULQ R8
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)

	// |
	MOVQ c+0(FP), DI
	MOVQ R9, (DI)
	MOVQ R10, 8(DI)
	MOVQ R11, 16(DI)
	MOVQ R12, 24(DI)
	MOVQ R13, 32(DI)
	MOVQ R14, 40(DI)
	MOVQ R15, 48(DI)
	MOVQ CX, 56(DI)
	MOVQ SI, 64(DI)
	MOVQ (SP), BX
	MOVQ BX, 72(DI)
	MOVQ 8(SP), BX
	MOVQ BX, 80(DI)
	MOVQ 16(SP), BX
	MOVQ BX, 88(DI)
	MOVQ 24(SP), BX
	MOVQ BX, 96(DI)
	MOVQ 32(SP), BX
	MOVQ BX, 104(DI)
	MOVQ 40(SP), BX
	MOVQ BX, 112(DI)
	MOVQ 48(SP), BX
	MOVQ BX, 120(DI)
	MOVQ 56(SP), BX
	MOVQ BX, 128(DI)
	MOVQ 64(SP), BX
	MOVQ BX, 136(DI)
	MOVQ 72(SP), BX
	MOVQ BX, 144(DI)
	MOVQ 80(SP), BX
	MOVQ BX, 152(DI)
	MOVQ 88(SP), BX
	MOVQ BX, 160(DI)
	MOVQ 96(SP), BX
	MOVQ BX, 168(DI)
	RET

// func mul12(c *[24]uint64, a *[12]uint64, b *[12]uint64, p *[12]uint64, inp uint64)
TEXT ·mul12(SB), $208-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)
	MOVQ $0x00000000, 96(SP)
	MOVQ $0x00000000, 104(SP)
	MOVQ $0x00000000, 112(SP)
	MOVQ $0x00000000, 120(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// | a9 * b0
	// | (w9, w10) @ (8(SP), 16(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)

	// | a10 * b0
	// | (w10, w11) @ (16(SP), 24(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// | a11 * b0
	// | (w11, w12) @ (24(SP), 32(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a9 * b1
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a10 * b1
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a11 * b1
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a9 * b2
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a10 * b2
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a11 * b2
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a9 * b3
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a10 * b3
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a11 * b3
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a9 * b4
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a10 * b4
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a11 * b4
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a9 * b5
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a10 * b5
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a11 * b5
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a9 * b6
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a10 * b6
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a11 * b6
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a9 * b7
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a10 * b7
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a11 * b7
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8 * b8
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b8
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a10 * b8
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a11 * b8
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// |
	// | b9
	MOVQ 72(SI), CX

	// | a0 * b9
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a1 * b9
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a2 * b9
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a3 * b9
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a4 * b9
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a5 * b9
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a6 * b9
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a7 * b9
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a8 * b9
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a9 * b9
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a10 * b9
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a11 * b9
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// |
	// | b10
	MOVQ 80(SI), CX

	// | a0 * b10
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a1 * b10
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a2 * b10
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a3 * b10
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a4 * b10
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a5 * b10
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a6 * b10
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a7 * b10
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a8 * b10
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a9 * b10
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a10 * b10
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a11 * b10
	// | (w21, w22, w23) @ (104(SP), 112(SP), 120(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)

	// |
	// | b11
	MOVQ 88(SI), CX

	// | a0 * b11
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a1 * b11
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a2 * b11
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a3 * b11
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a4 * b11
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a5 * b11
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a6 * b11
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a7 * b11
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a8 * b11
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a9 * b11
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a10 * b11
	// | (w21, w22, w23) @ (104(SP), 112(SP), 120(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a11 * b11
	// | (w22, w23) @ (112(SP), 120(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 128(SP)
	MOVQ R14, 136(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 136(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, DI
	ADDQ SI, 136(SP)
	ADCQ $0x00, DI

	// | w7 @ 128(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, SI
	ADDQ DI, 128(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	ADDQ SI, 32(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 40(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 136(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, SI
	ADDQ DI, 136(SP)
	ADCQ $0x00, SI

	// | w7 @ 128(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, DI
	ADDQ SI, 128(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ R8
	ADDQ SI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 136(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, DI
	ADDQ SI, 136(SP)
	ADCQ $0x00, DI

	// | w7 @ 128(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, SI
	ADDQ DI, 128(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ R8
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w14 @ R9
	ADDQ SI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 136(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, SI
	ADDQ DI, 136(SP)
	ADCQ $0x00, SI

	// | w7 @ 128(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, DI
	ADDQ SI, 128(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ R8
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w14 @ R9
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w15 @ R10
	ADDQ SI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 136(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, DI
	ADDQ SI, 136(SP)
	ADCQ $0x00, DI

	// | w7 @ 128(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, SI
	ADDQ DI, 128(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ R8
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w14 @ R9
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w15 @ R10
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w16 @ R11
	ADDQ SI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 136(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, SI
	ADDQ DI, 136(SP)
	ADCQ $0x00, SI

	// | w7 @ 128(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, DI
	ADDQ SI, 128(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w14 @ R9
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w15 @ R10
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w16 @ R11
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w17 @ R12
	ADDQ SI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(SP), R13

	// | (u @ CX) = (w6 @ 136(SP)) * inp
	MOVQ 136(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 136(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, DI

	// | w7 @ 128(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, SI
	ADDQ DI, 128(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w14 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w15 @ R10
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w16 @ R11
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w17 @ R12
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w18 @ R13
	ADDQ SI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(SP), BX
	MOVQ BX, 136(SP)

	// | (u @ CX) = (w7 @ 128(SP)) * inp
	MOVQ 128(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 128(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w14 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w15 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w16 @ R11
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w17 @ R12
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w18 @ R13
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w19 @ 136(SP)
	ADDQ SI, R15
	ADCQ R15, 136(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(SP), BX
	MOVQ BX, 128(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w14 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w15 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w16 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w17 @ R12
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w18 @ R13
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w19 @ 136(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, SI
	ADDQ DI, 136(SP)
	ADCQ $0x00, SI

	// | w20 @ 128(SP)
	ADDQ SI, R15
	ADCQ R15, 128(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(SP), BX
	MOVQ BX, (SP)

	// | (u @ CX) = (w9 @ 8(SP)) * inp
	MOVQ 8(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w14 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w15 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w16 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w17 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w18 @ R13
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w19 @ 136(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, DI
	ADDQ SI, 136(SP)
	ADCQ $0x00, DI

	// | w20 @ 128(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, SI
	ADDQ DI, 128(SP)
	ADCQ $0x00, SI

	// | w21 @ (SP)
	ADDQ SI, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 112(SP), BX
	MOVQ BX, 8(SP)

	// | (u @ CX) = (w10 @ 16(SP)) * inp
	MOVQ 16(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w14 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w15 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w16 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w17 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w18 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w19 @ 136(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, SI
	ADDQ DI, 136(SP)
	ADCQ $0x00, SI

	// | w20 @ 128(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, DI
	ADDQ SI, 128(SP)
	ADCQ $0x00, DI

	// | w21 @ (SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w22 @ 8(SP)
	ADDQ SI, R15
	ADCQ R15, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 120(SP), BX
	MOVQ BX, 16(SP)

	// | (u @ CX) = (w11 @ 24(SP)) * inp
	MOVQ 24(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w14 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w15 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w16 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w17 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w18 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w19 @ 136(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, DI
	ADDQ SI, 136(SP)
	ADCQ $0x00, DI

	// | w20 @ 128(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, SI
	ADDQ DI, 128(SP)
	ADCQ $0x00, SI

	// | w21 @ (SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w22 @ 8(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w23 @ 16(SP)
	ADDQ SI, R15
	ADCQ R15, 16(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 32(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, 24(SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 144(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 152(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 160(SP)
	MOVQ 136(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 168(SP)
	MOVQ 128(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 176(SP)
	MOVQ (SP), BX
	SBBQ 72(R14), BX
	MOVQ BX, 184(SP)
	MOVQ 8(SP), BX
	SBBQ 80(R14), BX
	MOVQ BX, 192(SP)
	MOVQ 16(SP), BX
	SBBQ 88(R14), BX
	MOVQ BX, 200(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 32(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC 24(SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 144(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 152(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 160(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    136(SP), BX
	CMOVQCC 168(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    128(SP), BX
	CMOVQCC 176(SP), BX
	MOVQ    BX, 64(DI)
	MOVQ    (SP), BX
	CMOVQCC 184(SP), BX
	MOVQ    BX, 72(DI)
	MOVQ    8(SP), BX
	CMOVQCC 192(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    16(SP), BX
	CMOVQCC 200(SP), BX
	MOVQ    BX, 88(DI)
	RET


// func mul13(c *[26]uint64, a *[13]uint64, b *[13]uint64, p *[13]uint64, inp uint64)
TEXT ·mul13(SB), $232-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)
	MOVQ $0x00000000, 96(SP)
	MOVQ $0x00000000, 104(SP)
	MOVQ $0x00000000, 112(SP)
	MOVQ $0x00000000, 120(SP)
	MOVQ $0x00000000, 128(SP)
	MOVQ $0x00000000, 136(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// | a9 * b0
	// | (w9, w10) @ (8(SP), 16(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)

	// | a10 * b0
	// | (w10, w11) @ (16(SP), 24(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// | a11 * b0
	// | (w11, w12) @ (24(SP), 32(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)

	// | a12 * b0
	// | (w12, w13) @ (32(SP), 40(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a9 * b1
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a10 * b1
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a11 * b1
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a12 * b1
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a9 * b2
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a10 * b2
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a11 * b2
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a12 * b2
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a9 * b3
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a10 * b3
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a11 * b3
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a12 * b3
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a9 * b4
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a10 * b4
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a11 * b4
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a12 * b4
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a9 * b5
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a10 * b5
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a11 * b5
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a12 * b5
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a9 * b6
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a10 * b6
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a11 * b6
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a12 * b6
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a9 * b7
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a10 * b7
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a11 * b7
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a12 * b7
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8 * b8
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b8
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a10 * b8
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a11 * b8
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a12 * b8
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// |
	// | b9
	MOVQ 72(SI), CX

	// | a0 * b9
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a1 * b9
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a2 * b9
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a3 * b9
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a4 * b9
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a5 * b9
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a6 * b9
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a7 * b9
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a8 * b9
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a9 * b9
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a10 * b9
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a11 * b9
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a12 * b9
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// |
	// | b10
	MOVQ 80(SI), CX

	// | a0 * b10
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a1 * b10
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a2 * b10
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a3 * b10
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a4 * b10
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a5 * b10
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a6 * b10
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a7 * b10
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a8 * b10
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a9 * b10
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a10 * b10
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a11 * b10
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a12 * b10
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// |
	// | b11
	MOVQ 88(SI), CX

	// | a0 * b11
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a1 * b11
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a2 * b11
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a3 * b11
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a4 * b11
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a5 * b11
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a6 * b11
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a7 * b11
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a8 * b11
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a9 * b11
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a10 * b11
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a11 * b11
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a12 * b11
	// | (w23, w24, w25) @ (120(SP), 128(SP), 136(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)

	// |
	// | b12
	MOVQ 96(SI), CX

	// | a0 * b12
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a1 * b12
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a2 * b12
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a3 * b12
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a4 * b12
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a5 * b12
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a6 * b12
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a7 * b12
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a8 * b12
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a9 * b12
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a10 * b12
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a11 * b12
	// | (w23, w24, w25) @ (120(SP), 128(SP), 136(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a12 * b12
	// | (w24, w25) @ (128(SP), 136(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 144(SP)
	MOVQ R14, 152(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 152(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, DI
	ADDQ SI, 152(SP)
	ADCQ $0x00, DI

	// | w7 @ 144(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, SI
	ADDQ DI, 144(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	ADDQ DI, 40(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 48(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 152(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, SI
	ADDQ DI, 152(SP)
	ADCQ $0x00, SI

	// | w7 @ 144(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, DI
	ADDQ SI, 144(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ R8
	ADDQ DI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 152(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, DI
	ADDQ SI, 152(SP)
	ADCQ $0x00, DI

	// | w7 @ 144(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, SI
	ADDQ DI, 144(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ R8
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w15 @ R9
	ADDQ DI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 152(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, SI
	ADDQ DI, 152(SP)
	ADCQ $0x00, SI

	// | w7 @ 144(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, DI
	ADDQ SI, 144(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ R8
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w15 @ R9
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w16 @ R10
	ADDQ DI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 152(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, DI
	ADDQ SI, 152(SP)
	ADCQ $0x00, DI

	// | w7 @ 144(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, SI
	ADDQ DI, 144(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ R8
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w15 @ R9
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w16 @ R10
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w17 @ R11
	ADDQ DI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 152(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, SI
	ADDQ DI, 152(SP)
	ADCQ $0x00, SI

	// | w7 @ 144(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, DI
	ADDQ SI, 144(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ R8
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w15 @ R9
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w16 @ R10
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w17 @ R11
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w18 @ R12
	ADDQ DI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(SP), R13

	// | (u @ CX) = (w6 @ 152(SP)) * inp
	MOVQ 152(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 152(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, DI

	// | w7 @ 144(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, SI
	ADDQ DI, 144(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w15 @ R9
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w16 @ R10
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w17 @ R11
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w18 @ R12
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w19 @ R13
	ADDQ DI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(SP), BX
	MOVQ BX, 152(SP)

	// | (u @ CX) = (w7 @ 144(SP)) * inp
	MOVQ 144(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 144(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w15 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w16 @ R10
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w17 @ R11
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w18 @ R12
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w19 @ R13
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w20 @ 152(SP)
	ADDQ DI, R15
	ADCQ R15, 152(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(SP), BX
	MOVQ BX, 144(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w15 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w16 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w17 @ R11
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w18 @ R12
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w19 @ R13
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w20 @ 152(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, DI
	ADDQ SI, 152(SP)
	ADCQ $0x00, DI

	// | w21 @ 144(SP)
	ADDQ DI, R15
	ADCQ R15, 144(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 112(SP), BX
	MOVQ BX, (SP)

	// | (u @ CX) = (w9 @ 8(SP)) * inp
	MOVQ 8(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w15 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w16 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w17 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w18 @ R12
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w19 @ R13
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w20 @ 152(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, SI
	ADDQ DI, 152(SP)
	ADCQ $0x00, SI

	// | w21 @ 144(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, DI
	ADDQ SI, 144(SP)
	ADCQ $0x00, DI

	// | w22 @ (SP)
	ADDQ DI, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 120(SP), BX
	MOVQ BX, 8(SP)

	// | (u @ CX) = (w10 @ 16(SP)) * inp
	MOVQ 16(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w15 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w16 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w17 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w18 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w19 @ R13
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w20 @ 152(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, DI
	ADDQ SI, 152(SP)
	ADCQ $0x00, DI

	// | w21 @ 144(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, SI
	ADDQ DI, 144(SP)
	ADCQ $0x00, SI

	// | w22 @ (SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w23 @ 8(SP)
	ADDQ DI, R15
	ADCQ R15, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 128(SP), BX
	MOVQ BX, 16(SP)

	// | (u @ CX) = (w11 @ 24(SP)) * inp
	MOVQ 24(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w15 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w16 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w17 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w18 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w19 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w20 @ 152(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, SI
	ADDQ DI, 152(SP)
	ADCQ $0x00, SI

	// | w21 @ 144(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, DI
	ADDQ SI, 144(SP)
	ADCQ $0x00, DI

	// | w22 @ (SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w23 @ 8(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w24 @ 16(SP)
	ADDQ DI, R15
	ADCQ R15, 16(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 136(SP), BX
	MOVQ BX, 24(SP)

	// | (u @ CX) = (w12 @ 32(SP)) * inp
	MOVQ 32(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w15 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w16 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w17 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w18 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w19 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w20 @ 152(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, DI
	ADDQ SI, 152(SP)
	ADCQ $0x00, DI

	// | w21 @ 144(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, SI
	ADDQ DI, 144(SP)
	ADCQ $0x00, SI

	// | w22 @ (SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w23 @ 8(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w24 @ 16(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w25 @ 24(SP)
	ADDQ DI, R15
	ADCQ R15, 24(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 40(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, 32(SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 160(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 168(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 176(SP)
	MOVQ 152(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 184(SP)
	MOVQ 144(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 192(SP)
	MOVQ (SP), BX
	SBBQ 72(R14), BX
	MOVQ BX, 200(SP)
	MOVQ 8(SP), BX
	SBBQ 80(R14), BX
	MOVQ BX, 208(SP)
	MOVQ 16(SP), BX
	SBBQ 88(R14), BX
	MOVQ BX, 216(SP)
	MOVQ 24(SP), BX
	SBBQ 96(R14), BX
	MOVQ BX, 224(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 40(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC 32(SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 160(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 168(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 176(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    152(SP), BX
	CMOVQCC 184(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    144(SP), BX
	CMOVQCC 192(SP), BX
	MOVQ    BX, 64(DI)
	MOVQ    (SP), BX
	CMOVQCC 200(SP), BX
	MOVQ    BX, 72(DI)
	MOVQ    8(SP), BX
	CMOVQCC 208(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    16(SP), BX
	CMOVQCC 216(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    24(SP), BX
	CMOVQCC 224(SP), BX
	MOVQ    BX, 96(DI)
	RET


// func mul14(c *[28]uint64, a *[14]uint64, b *[14]uint64, p *[14]uint64, inp uint64)
TEXT ·mul14(SB), $256-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)
	MOVQ $0x00000000, 96(SP)
	MOVQ $0x00000000, 104(SP)
	MOVQ $0x00000000, 112(SP)
	MOVQ $0x00000000, 120(SP)
	MOVQ $0x00000000, 128(SP)
	MOVQ $0x00000000, 136(SP)
	MOVQ $0x00000000, 144(SP)
	MOVQ $0x00000000, 152(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// | a9 * b0
	// | (w9, w10) @ (8(SP), 16(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)

	// | a10 * b0
	// | (w10, w11) @ (16(SP), 24(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// | a11 * b0
	// | (w11, w12) @ (24(SP), 32(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)

	// | a12 * b0
	// | (w12, w13) @ (32(SP), 40(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)

	// | a13 * b0
	// | (w13, w14) @ (40(SP), 48(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a9 * b1
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a10 * b1
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a11 * b1
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a12 * b1
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a13 * b1
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a9 * b2
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a10 * b2
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a11 * b2
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a12 * b2
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a13 * b2
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a9 * b3
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a10 * b3
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a11 * b3
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a12 * b3
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a13 * b3
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a9 * b4
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a10 * b4
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a11 * b4
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a12 * b4
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a13 * b4
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a9 * b5
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a10 * b5
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a11 * b5
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a12 * b5
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a13 * b5
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a9 * b6
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a10 * b6
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a11 * b6
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a12 * b6
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a13 * b6
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a9 * b7
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a10 * b7
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a11 * b7
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a12 * b7
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a13 * b7
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8 * b8
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b8
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a10 * b8
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a11 * b8
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a12 * b8
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a13 * b8
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// |
	// | b9
	MOVQ 72(SI), CX

	// | a0 * b9
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a1 * b9
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a2 * b9
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a3 * b9
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a4 * b9
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a5 * b9
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a6 * b9
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a7 * b9
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a8 * b9
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a9 * b9
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a10 * b9
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a11 * b9
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a12 * b9
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a13 * b9
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// |
	// | b10
	MOVQ 80(SI), CX

	// | a0 * b10
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a1 * b10
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a2 * b10
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a3 * b10
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a4 * b10
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a5 * b10
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a6 * b10
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a7 * b10
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a8 * b10
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a9 * b10
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a10 * b10
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a11 * b10
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a12 * b10
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a13 * b10
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// |
	// | b11
	MOVQ 88(SI), CX

	// | a0 * b11
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a1 * b11
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a2 * b11
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a3 * b11
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a4 * b11
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a5 * b11
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a6 * b11
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a7 * b11
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a8 * b11
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a9 * b11
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a10 * b11
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a11 * b11
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a12 * b11
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a13 * b11
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// |
	// | b12
	MOVQ 96(SI), CX

	// | a0 * b12
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a1 * b12
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a2 * b12
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a3 * b12
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a4 * b12
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a5 * b12
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a6 * b12
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a7 * b12
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a8 * b12
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a9 * b12
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a10 * b12
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a11 * b12
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a12 * b12
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a13 * b12
	// | (w25, w26, w27) @ (136(SP), 144(SP), 152(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)

	// |
	// | b13
	MOVQ 104(SI), CX

	// | a0 * b13
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a1 * b13
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a2 * b13
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a3 * b13
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a4 * b13
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a5 * b13
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a6 * b13
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a7 * b13
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a8 * b13
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a9 * b13
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a10 * b13
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a11 * b13
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a12 * b13
	// | (w25, w26, w27) @ (136(SP), 144(SP), 152(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a13 * b13
	// | (w26, w27) @ (144(SP), 152(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 160(SP)
	MOVQ R14, 168(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 168(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, DI
	ADDQ SI, 168(SP)
	ADCQ $0x00, DI

	// | w7 @ 160(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, SI
	ADDQ DI, 160(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	ADDQ SI, 48(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 56(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 168(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, SI
	ADDQ DI, 168(SP)
	ADCQ $0x00, SI

	// | w7 @ 160(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, DI
	ADDQ SI, 160(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ R8
	ADDQ SI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 168(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, DI
	ADDQ SI, 168(SP)
	ADCQ $0x00, DI

	// | w7 @ 160(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, SI
	ADDQ DI, 160(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ R8
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w16 @ R9
	ADDQ SI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 168(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, SI
	ADDQ DI, 168(SP)
	ADCQ $0x00, SI

	// | w7 @ 160(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, DI
	ADDQ SI, 160(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ R8
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w16 @ R9
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w17 @ R10
	ADDQ SI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 168(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, DI
	ADDQ SI, 168(SP)
	ADCQ $0x00, DI

	// | w7 @ 160(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, SI
	ADDQ DI, 160(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ R8
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w16 @ R9
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w17 @ R10
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w18 @ R11
	ADDQ SI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 168(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, SI
	ADDQ DI, 168(SP)
	ADCQ $0x00, SI

	// | w7 @ 160(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, DI
	ADDQ SI, 160(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ R8
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w16 @ R9
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w17 @ R10
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w18 @ R11
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w19 @ R12
	ADDQ SI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(SP), R13

	// | (u @ CX) = (w6 @ 168(SP)) * inp
	MOVQ 168(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 168(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, DI

	// | w7 @ 160(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, SI
	ADDQ DI, 160(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ R8
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w16 @ R9
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w17 @ R10
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w18 @ R11
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w19 @ R12
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w20 @ R13
	ADDQ SI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(SP), BX
	MOVQ BX, 168(SP)

	// | (u @ CX) = (w7 @ 160(SP)) * inp
	MOVQ 160(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 160(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w16 @ R9
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w17 @ R10
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w18 @ R11
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w19 @ R12
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w20 @ R13
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w21 @ 168(SP)
	ADDQ SI, R15
	ADCQ R15, 168(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 112(SP), BX
	MOVQ BX, 160(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w16 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w17 @ R10
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w18 @ R11
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w19 @ R12
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w20 @ R13
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w21 @ 168(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, SI
	ADDQ DI, 168(SP)
	ADCQ $0x00, SI

	// | w22 @ 160(SP)
	ADDQ SI, R15
	ADCQ R15, 160(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 120(SP), BX
	MOVQ BX, (SP)

	// | (u @ CX) = (w9 @ 8(SP)) * inp
	MOVQ 8(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w16 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w17 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w18 @ R11
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w19 @ R12
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w20 @ R13
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w21 @ 168(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, DI
	ADDQ SI, 168(SP)
	ADCQ $0x00, DI

	// | w22 @ 160(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, SI
	ADDQ DI, 160(SP)
	ADCQ $0x00, SI

	// | w23 @ (SP)
	ADDQ SI, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 128(SP), BX
	MOVQ BX, 8(SP)

	// | (u @ CX) = (w10 @ 16(SP)) * inp
	MOVQ 16(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w16 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w17 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w18 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w19 @ R12
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w20 @ R13
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w21 @ 168(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, SI
	ADDQ DI, 168(SP)
	ADCQ $0x00, SI

	// | w22 @ 160(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, DI
	ADDQ SI, 160(SP)
	ADCQ $0x00, DI

	// | w23 @ (SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w24 @ 8(SP)
	ADDQ SI, R15
	ADCQ R15, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 136(SP), BX
	MOVQ BX, 16(SP)

	// | (u @ CX) = (w11 @ 24(SP)) * inp
	MOVQ 24(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w16 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w17 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w18 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w19 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w20 @ R13
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w21 @ 168(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, DI
	ADDQ SI, 168(SP)
	ADCQ $0x00, DI

	// | w22 @ 160(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, SI
	ADDQ DI, 160(SP)
	ADCQ $0x00, SI

	// | w23 @ (SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w24 @ 8(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w25 @ 16(SP)
	ADDQ SI, R15
	ADCQ R15, 16(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 144(SP), BX
	MOVQ BX, 24(SP)

	// | (u @ CX) = (w12 @ 32(SP)) * inp
	MOVQ 32(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w16 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w17 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w18 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w19 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w20 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w21 @ 168(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, SI
	ADDQ DI, 168(SP)
	ADCQ $0x00, SI

	// | w22 @ 160(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, DI
	ADDQ SI, 160(SP)
	ADCQ $0x00, DI

	// | w23 @ (SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w24 @ 8(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w25 @ 16(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w26 @ 24(SP)
	ADDQ SI, R15
	ADCQ R15, 24(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 152(SP), BX
	MOVQ BX, 32(SP)

	// | (u @ CX) = (w13 @ 40(SP)) * inp
	MOVQ 40(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w16 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w17 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w18 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w19 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w20 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w21 @ 168(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, DI
	ADDQ SI, 168(SP)
	ADCQ $0x00, DI

	// | w22 @ 160(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, SI
	ADDQ DI, 160(SP)
	ADCQ $0x00, SI

	// | w23 @ (SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w24 @ 8(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w25 @ 16(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w26 @ 24(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w27 @ 32(SP)
	ADDQ SI, R15
	ADCQ R15, 32(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 48(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, 40(SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 176(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 184(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 192(SP)
	MOVQ 168(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 200(SP)
	MOVQ 160(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 208(SP)
	MOVQ (SP), BX
	SBBQ 72(R14), BX
	MOVQ BX, 216(SP)
	MOVQ 8(SP), BX
	SBBQ 80(R14), BX
	MOVQ BX, 224(SP)
	MOVQ 16(SP), BX
	SBBQ 88(R14), BX
	MOVQ BX, 232(SP)
	MOVQ 24(SP), BX
	SBBQ 96(R14), BX
	MOVQ BX, 240(SP)
	MOVQ 32(SP), BX
	SBBQ 104(R14), BX
	MOVQ BX, 248(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 48(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC 40(SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 176(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 184(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 192(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    168(SP), BX
	CMOVQCC 200(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    160(SP), BX
	CMOVQCC 208(SP), BX
	MOVQ    BX, 64(DI)
	MOVQ    (SP), BX
	CMOVQCC 216(SP), BX
	MOVQ    BX, 72(DI)
	MOVQ    8(SP), BX
	CMOVQCC 224(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    16(SP), BX
	CMOVQCC 232(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    24(SP), BX
	CMOVQCC 240(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    32(SP), BX
	CMOVQCC 248(SP), BX
	MOVQ    BX, 104(DI)
	RET


// func mul12(c *[30]uint64, a *[15]uint64, b *[15]uint64, p *[15]uint64, inp uint64)
TEXT ·mul15(SB), $280-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)
	MOVQ $0x00000000, 96(SP)
	MOVQ $0x00000000, 104(SP)
	MOVQ $0x00000000, 112(SP)
	MOVQ $0x00000000, 120(SP)
	MOVQ $0x00000000, 128(SP)
	MOVQ $0x00000000, 136(SP)
	MOVQ $0x00000000, 144(SP)
	MOVQ $0x00000000, 152(SP)
	MOVQ $0x00000000, 160(SP)
	MOVQ $0x00000000, 168(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// | a9 * b0
	// | (w9, w10) @ (8(SP), 16(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)

	// | a10 * b0
	// | (w10, w11) @ (16(SP), 24(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// | a11 * b0
	// | (w11, w12) @ (24(SP), 32(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)

	// | a12 * b0
	// | (w12, w13) @ (32(SP), 40(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)

	// | a13 * b0
	// | (w13, w14) @ (40(SP), 48(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)

	// | a14 * b0
	// | (w14, w15) @ (48(SP), 56(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a9 * b1
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a10 * b1
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a11 * b1
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a12 * b1
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a13 * b1
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a14 * b1
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a9 * b2
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a10 * b2
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a11 * b2
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a12 * b2
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a13 * b2
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a14 * b2
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a9 * b3
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a10 * b3
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a11 * b3
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a12 * b3
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a13 * b3
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a14 * b3
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a9 * b4
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a10 * b4
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a11 * b4
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a12 * b4
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a13 * b4
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a14 * b4
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a9 * b5
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a10 * b5
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a11 * b5
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a12 * b5
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a13 * b5
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a14 * b5
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a9 * b6
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a10 * b6
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a11 * b6
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a12 * b6
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a13 * b6
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a14 * b6
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a9 * b7
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a10 * b7
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a11 * b7
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a12 * b7
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a13 * b7
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a14 * b7
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8 * b8
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b8
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a10 * b8
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a11 * b8
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a12 * b8
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a13 * b8
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a14 * b8
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// |
	// | b9
	MOVQ 72(SI), CX

	// | a0 * b9
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a1 * b9
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a2 * b9
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a3 * b9
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a4 * b9
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a5 * b9
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a6 * b9
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a7 * b9
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a8 * b9
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a9 * b9
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a10 * b9
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a11 * b9
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a12 * b9
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a13 * b9
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a14 * b9
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// |
	// | b10
	MOVQ 80(SI), CX

	// | a0 * b10
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a1 * b10
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a2 * b10
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a3 * b10
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a4 * b10
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a5 * b10
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a6 * b10
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a7 * b10
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a8 * b10
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a9 * b10
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a10 * b10
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a11 * b10
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a12 * b10
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a13 * b10
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a14 * b10
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// |
	// | b11
	MOVQ 88(SI), CX

	// | a0 * b11
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a1 * b11
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a2 * b11
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a3 * b11
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a4 * b11
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a5 * b11
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a6 * b11
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a7 * b11
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a8 * b11
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a9 * b11
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a10 * b11
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a11 * b11
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a12 * b11
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a13 * b11
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a14 * b11
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// |
	// | b12
	MOVQ 96(SI), CX

	// | a0 * b12
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a1 * b12
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a2 * b12
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a3 * b12
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a4 * b12
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a5 * b12
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a6 * b12
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a7 * b12
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a8 * b12
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a9 * b12
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a10 * b12
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a11 * b12
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a12 * b12
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a13 * b12
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a14 * b12
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// |
	// | b13
	MOVQ 104(SI), CX

	// | a0 * b13
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a1 * b13
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a2 * b13
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a3 * b13
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a4 * b13
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a5 * b13
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a6 * b13
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a7 * b13
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a8 * b13
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a9 * b13
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a10 * b13
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a11 * b13
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a12 * b13
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a13 * b13
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// | a14 * b13
	// | (w27, w28, w29) @ (152(SP), 160(SP), 168(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, 160(SP)
	ADCQ $0x00, 168(SP)

	// |
	// | b14
	MOVQ 112(SI), CX

	// | a0 * b14
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a1 * b14
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a2 * b14
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a3 * b14
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a4 * b14
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a5 * b14
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a6 * b14
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a7 * b14
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a8 * b14
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a9 * b14
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a10 * b14
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a11 * b14
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a12 * b14
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// | a13 * b14
	// | (w27, w28, w29) @ (152(SP), 160(SP), 168(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, 160(SP)
	ADCQ $0x00, 168(SP)

	// | a14 * b14
	// | (w28, w29) @ (160(SP), 168(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, 168(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 176(SP)
	MOVQ R14, 184(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 184(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI
	ADDQ SI, 184(SP)
	ADCQ $0x00, DI

	// | w7 @ 176(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, SI
	ADDQ DI, 176(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	ADDQ DI, 56(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 64(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 184(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, SI
	ADDQ DI, 184(SP)
	ADCQ $0x00, SI

	// | w7 @ 176(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, DI
	ADDQ SI, 176(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ R8
	ADDQ DI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 184(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI
	ADDQ SI, 184(SP)
	ADCQ $0x00, DI

	// | w7 @ 176(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, SI
	ADDQ DI, 176(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ R8
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w17 @ R9
	ADDQ DI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 184(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, SI
	ADDQ DI, 184(SP)
	ADCQ $0x00, SI

	// | w7 @ 176(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, DI
	ADDQ SI, 176(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ R8
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w17 @ R9
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w18 @ R10
	ADDQ DI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 184(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI
	ADDQ SI, 184(SP)
	ADCQ $0x00, DI

	// | w7 @ 176(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, SI
	ADDQ DI, 176(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ R8
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w17 @ R9
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w18 @ R10
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w19 @ R11
	ADDQ DI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 184(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, SI
	ADDQ DI, 184(SP)
	ADCQ $0x00, SI

	// | w7 @ 176(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, DI
	ADDQ SI, 176(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ R8
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w17 @ R9
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w18 @ R10
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w19 @ R11
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w20 @ R12
	ADDQ DI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(SP), R13

	// | (u @ CX) = (w6 @ 184(SP)) * inp
	MOVQ 184(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 184(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI

	// | w7 @ 176(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, SI
	ADDQ DI, 176(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ R8
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w17 @ R9
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w18 @ R10
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w19 @ R11
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w20 @ R12
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w21 @ R13
	ADDQ DI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 112(SP), BX
	MOVQ BX, 184(SP)

	// | (u @ CX) = (w7 @ 176(SP)) * inp
	MOVQ 176(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 176(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ R8
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w17 @ R9
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w18 @ R10
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w19 @ R11
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w20 @ R12
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w21 @ R13
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w22 @ 184(SP)
	ADDQ DI, R15
	ADCQ R15, 184(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 120(SP), BX
	MOVQ BX, 176(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w17 @ R9
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w18 @ R10
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w19 @ R11
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w20 @ R12
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w21 @ R13
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w22 @ 184(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI
	ADDQ SI, 184(SP)
	ADCQ $0x00, DI

	// | w23 @ 176(SP)
	ADDQ DI, R15
	ADCQ R15, 176(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 128(SP), BX
	MOVQ BX, (SP)

	// | (u @ CX) = (w9 @ 8(SP)) * inp
	MOVQ 8(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w17 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w18 @ R10
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w19 @ R11
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w20 @ R12
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w21 @ R13
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w22 @ 184(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, SI
	ADDQ DI, 184(SP)
	ADCQ $0x00, SI

	// | w23 @ 176(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, DI
	ADDQ SI, 176(SP)
	ADCQ $0x00, DI

	// | w24 @ (SP)
	ADDQ DI, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 136(SP), BX
	MOVQ BX, 8(SP)

	// | (u @ CX) = (w10 @ 16(SP)) * inp
	MOVQ 16(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w17 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w18 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w19 @ R11
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w20 @ R12
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w21 @ R13
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w22 @ 184(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI
	ADDQ SI, 184(SP)
	ADCQ $0x00, DI

	// | w23 @ 176(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, SI
	ADDQ DI, 176(SP)
	ADCQ $0x00, SI

	// | w24 @ (SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w25 @ 8(SP)
	ADDQ DI, R15
	ADCQ R15, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 144(SP), BX
	MOVQ BX, 16(SP)

	// | (u @ CX) = (w11 @ 24(SP)) * inp
	MOVQ 24(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w17 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w18 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w19 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w20 @ R12
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w21 @ R13
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w22 @ 184(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, SI
	ADDQ DI, 184(SP)
	ADCQ $0x00, SI

	// | w23 @ 176(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, DI
	ADDQ SI, 176(SP)
	ADCQ $0x00, DI

	// | w24 @ (SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w25 @ 8(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w26 @ 16(SP)
	ADDQ DI, R15
	ADCQ R15, 16(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 152(SP), BX
	MOVQ BX, 24(SP)

	// | (u @ CX) = (w12 @ 32(SP)) * inp
	MOVQ 32(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w17 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w18 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w19 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w20 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w21 @ R13
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w22 @ 184(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI
	ADDQ SI, 184(SP)
	ADCQ $0x00, DI

	// | w23 @ 176(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, SI
	ADDQ DI, 176(SP)
	ADCQ $0x00, SI

	// | w24 @ (SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w25 @ 8(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w26 @ 16(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w27 @ 24(SP)
	ADDQ DI, R15
	ADCQ R15, 24(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 160(SP), BX
	MOVQ BX, 32(SP)

	// | (u @ CX) = (w13 @ 40(SP)) * inp
	MOVQ 40(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w17 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w18 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w19 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w20 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w21 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w22 @ 184(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, SI
	ADDQ DI, 184(SP)
	ADCQ $0x00, SI

	// | w23 @ 176(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, DI
	ADDQ SI, 176(SP)
	ADCQ $0x00, DI

	// | w24 @ (SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w25 @ 8(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w26 @ 16(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w27 @ 24(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w28 @ 32(SP)
	ADDQ DI, R15
	ADCQ R15, 32(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 168(SP), BX
	MOVQ BX, 40(SP)

	// | (u @ CX) = (w14 @ 48(SP)) * inp
	MOVQ 48(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w17 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w18 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w19 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w20 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w21 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w22 @ 184(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 184(SP)
	ADCQ DX, DI
	ADDQ SI, 184(SP)
	ADCQ $0x00, DI

	// | w23 @ 176(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, SI
	ADDQ DI, 176(SP)
	ADCQ $0x00, SI

	// | w24 @ (SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w25 @ 8(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w26 @ 16(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w27 @ 24(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w28 @ 32(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w29 @ 40(SP)
	ADDQ DI, R15
	ADCQ R15, 40(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 56(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, 48(SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 192(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 200(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 208(SP)
	MOVQ 184(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 216(SP)
	MOVQ 176(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 224(SP)
	MOVQ (SP), BX
	SBBQ 72(R14), BX
	MOVQ BX, 232(SP)
	MOVQ 8(SP), BX
	SBBQ 80(R14), BX
	MOVQ BX, 240(SP)
	MOVQ 16(SP), BX
	SBBQ 88(R14), BX
	MOVQ BX, 248(SP)
	MOVQ 24(SP), BX
	SBBQ 96(R14), BX
	MOVQ BX, 256(SP)
	MOVQ 32(SP), BX
	SBBQ 104(R14), BX
	MOVQ BX, 264(SP)
	MOVQ 40(SP), BX
	SBBQ 112(R14), BX
	MOVQ BX, 272(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 56(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC 48(SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 192(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 200(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 208(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    184(SP), BX
	CMOVQCC 216(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    176(SP), BX
	CMOVQCC 224(SP), BX
	MOVQ    BX, 64(DI)
	MOVQ    (SP), BX
	CMOVQCC 232(SP), BX
	MOVQ    BX, 72(DI)
	MOVQ    8(SP), BX
	CMOVQCC 240(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    16(SP), BX
	CMOVQCC 248(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    24(SP), BX
	CMOVQCC 256(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    32(SP), BX
	CMOVQCC 264(SP), BX
	MOVQ    BX, 104(DI)
	MOVQ    40(SP), BX
	CMOVQCC 272(SP), BX
	MOVQ    BX, 112(DI)
	RET


// func mul12(c *[32]uint64, a *[16]uint64, b *[16]uint64, p *[16]uint64, inp uint64)
TEXT ·mul16(SB), $304-40
/* inputs 				*/
	// |
	// | Multiplication
	MOVQ a+8(FP), DI
	MOVQ b+16(FP), SI

	// |
	// |
	XORQ R10, R10
	XORQ R11, R11
	XORQ R12, R12
	XORQ R13, R13
	XORQ R14, R14
	XORQ R15, R15
	MOVQ $0x00000000, (SP)
	MOVQ $0x00000000, 8(SP)
	MOVQ $0x00000000, 16(SP)
	MOVQ $0x00000000, 24(SP)
	MOVQ $0x00000000, 32(SP)
	MOVQ $0x00000000, 40(SP)
	MOVQ $0x00000000, 48(SP)
	MOVQ $0x00000000, 56(SP)
	MOVQ $0x00000000, 64(SP)
	MOVQ $0x00000000, 72(SP)
	MOVQ $0x00000000, 80(SP)
	MOVQ $0x00000000, 88(SP)
	MOVQ $0x00000000, 96(SP)
	MOVQ $0x00000000, 104(SP)
	MOVQ $0x00000000, 112(SP)
	MOVQ $0x00000000, 120(SP)
	MOVQ $0x00000000, 128(SP)
	MOVQ $0x00000000, 136(SP)
	MOVQ $0x00000000, 144(SP)
	MOVQ $0x00000000, 152(SP)
	MOVQ $0x00000000, 160(SP)
	MOVQ $0x00000000, 168(SP)
	MOVQ $0x00000000, 176(SP)
	MOVQ $0x00000000, 184(SP)

	// |
	// | b0
	MOVQ (SI), CX

	// | a0 * b0
	// | (w0, w1) @ (R8, R9)
	MOVQ (DI), AX
	MULQ CX
	MOVQ AX, R8
	MOVQ DX, R9

	// | a1 * b0
	// | (w1, w2) @ (R9, R10)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10

	// | a2 * b0
	// | (w2, w3) @ (R10, R11)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11

	// | a3 * b0
	// | (w3, w4) @ (R11, R12)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12

	// | a4 * b0
	// | (w4, w5) @ (R12, R13)
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13

	// | a5 * b0
	// | (w5, w6) @ (R13, R14)
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14

	// | a6 * b0
	// | (w6, w7) @ (R14, R15)
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15

	// | a7 * b0
	// | (w7, w8) @ (R15, (SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)

	// | a8 * b0
	// | (w8, w9) @ ((SP), 8(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)

	// | a9 * b0
	// | (w9, w10) @ (8(SP), 16(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)

	// | a10 * b0
	// | (w10, w11) @ (16(SP), 24(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)

	// | a11 * b0
	// | (w11, w12) @ (24(SP), 32(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)

	// | a12 * b0
	// | (w12, w13) @ (32(SP), 40(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)

	// | a13 * b0
	// | (w13, w14) @ (40(SP), 48(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)

	// | a14 * b0
	// | (w14, w15) @ (48(SP), 56(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)

	// | a15 * b0
	// | (w15, w16) @ (56(SP), 64(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)

	// |
	// | b1
	MOVQ 8(SI), CX

	// | a0 * b1
	// | (w1, w2, w3, w4) @ (R9, R10, R11, R12)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, R10
	ADCQ $0x00, R11
	ADCQ $0x00, R12

	// | a1 * b1
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a2 * b1
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a3 * b1
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a4 * b1
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a5 * b1
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a6 * b1
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a7 * b1
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a8 * b1
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a9 * b1
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a10 * b1
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a11 * b1
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a12 * b1
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a13 * b1
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a14 * b1
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a15 * b1
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// |
	// | b2
	MOVQ 16(SI), CX

	// | a0 * b2
	// | (w2, w3, w4, w5) @ (R10, R11, R12, R13)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, R11
	ADCQ $0x00, R12
	ADCQ $0x00, R13

	// | a1 * b2
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a2 * b2
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a3 * b2
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a4 * b2
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a5 * b2
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a6 * b2
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a7 * b2
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a8 * b2
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a9 * b2
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a10 * b2
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a11 * b2
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a12 * b2
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a13 * b2
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a14 * b2
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a15 * b2
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// |
	// | b3
	MOVQ 24(SI), CX

	// | a0 * b3
	// | (w3, w4, w5, w6) @ (R11, R12, R13, R14)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, R12
	ADCQ $0x00, R13
	ADCQ $0x00, R14

	// | a1 * b3
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a2 * b3
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a3 * b3
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a4 * b3
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a5 * b3
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a6 * b3
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a7 * b3
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a8 * b3
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a9 * b3
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a10 * b3
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a11 * b3
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a12 * b3
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a13 * b3
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a14 * b3
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a15 * b3
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// |
	// | b4
	MOVQ 32(SI), CX

	// | a0 * b4
	// | (w4, w5, w6, w7) @ (R12, R13, R14, R15)
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, R13
	ADCQ $0x00, R14
	ADCQ $0x00, R15

	// | a1 * b4
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a2 * b4
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a3 * b4
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a4 * b4
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a5 * b4
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a6 * b4
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a7 * b4
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a8 * b4
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a9 * b4
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a10 * b4
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a11 * b4
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a12 * b4
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a13 * b4
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a14 * b4
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a15 * b4
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// |
	// | b5
	MOVQ 40(SI), CX

	// | a0 * b5
	// | (w5, w6, w7, w8) @ (R13, R14, R15, (SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, R14
	ADCQ $0x00, R15
	ADCQ $0x00, (SP)

	// | a1 * b5
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a2 * b5
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a3 * b5
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a4 * b5
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a5 * b5
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a6 * b5
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a7 * b5
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a8 * b5
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a9 * b5
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a10 * b5
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a11 * b5
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a12 * b5
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a13 * b5
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a14 * b5
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a15 * b5
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// |
	// | b6
	MOVQ 48(SI), CX

	// | a0 * b6
	// | (w6, w7, w8, w9) @ (R14, R15, (SP), 8(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R14
	ADCQ DX, R15
	ADCQ $0x00, (SP)
	ADCQ $0x00, 8(SP)

	// | a1 * b6
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a2 * b6
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a3 * b6
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a4 * b6
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a5 * b6
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a6 * b6
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a7 * b6
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a8 * b6
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a9 * b6
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a10 * b6
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a11 * b6
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a12 * b6
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a13 * b6
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a14 * b6
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a15 * b6
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// |
	// | b7
	MOVQ 56(SI), CX

	// | a0 * b7
	// | (w7, w8, w9, w10) @ (R15, (SP), 8(SP), 16(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, R15
	ADCQ DX, (SP)
	ADCQ $0x00, 8(SP)
	ADCQ $0x00, 16(SP)

	// | a1 * b7
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a2 * b7
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a3 * b7
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a4 * b7
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a5 * b7
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a6 * b7
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a7 * b7
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a8 * b7
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a9 * b7
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a10 * b7
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a11 * b7
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a12 * b7
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a13 * b7
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a14 * b7
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a15 * b7
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// |
	// | b8
	MOVQ 64(SI), CX

	// | a0 * b8
	// | (w8, w9, w10, w11) @ ((SP), 8(SP), 16(SP), 24(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, 8(SP)
	ADCQ $0x00, 16(SP)
	ADCQ $0x00, 24(SP)

	// | a1 * b8
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a2 * b8
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a3 * b8
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a4 * b8
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a5 * b8
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a6 * b8
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a7 * b8
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a8 * b8
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a9 * b8
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a10 * b8
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a11 * b8
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a12 * b8
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a13 * b8
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a14 * b8
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a15 * b8
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// |
	// | b9
	MOVQ 72(SI), CX

	// | a0 * b9
	// | (w9, w10, w11, w12) @ (8(SP), 16(SP), 24(SP), 32(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, 16(SP)
	ADCQ $0x00, 24(SP)
	ADCQ $0x00, 32(SP)

	// | a1 * b9
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a2 * b9
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a3 * b9
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a4 * b9
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a5 * b9
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a6 * b9
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a7 * b9
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a8 * b9
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a9 * b9
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a10 * b9
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a11 * b9
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a12 * b9
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a13 * b9
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a14 * b9
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a15 * b9
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// |
	// | b10
	MOVQ 80(SI), CX

	// | a0 * b10
	// | (w10, w11, w12, w13) @ (16(SP), 24(SP), 32(SP), 40(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, 24(SP)
	ADCQ $0x00, 32(SP)
	ADCQ $0x00, 40(SP)

	// | a1 * b10
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a2 * b10
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a3 * b10
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a4 * b10
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a5 * b10
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a6 * b10
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a7 * b10
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a8 * b10
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a9 * b10
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a10 * b10
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a11 * b10
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a12 * b10
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a13 * b10
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a14 * b10
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a15 * b10
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// |
	// | b11
	MOVQ 88(SI), CX

	// | a0 * b11
	// | (w11, w12, w13, w14) @ (24(SP), 32(SP), 40(SP), 48(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, 32(SP)
	ADCQ $0x00, 40(SP)
	ADCQ $0x00, 48(SP)

	// | a1 * b11
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a2 * b11
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a3 * b11
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a4 * b11
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a5 * b11
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a6 * b11
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a7 * b11
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a8 * b11
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a9 * b11
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a10 * b11
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a11 * b11
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a12 * b11
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a13 * b11
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a14 * b11
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a15 * b11
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// |
	// | b12
	MOVQ 96(SI), CX

	// | a0 * b12
	// | (w12, w13, w14, w15) @ (32(SP), 40(SP), 48(SP), 56(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, 40(SP)
	ADCQ $0x00, 48(SP)
	ADCQ $0x00, 56(SP)

	// | a1 * b12
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a2 * b12
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a3 * b12
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a4 * b12
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a5 * b12
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a6 * b12
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a7 * b12
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a8 * b12
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a9 * b12
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a10 * b12
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a11 * b12
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a12 * b12
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a13 * b12
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a14 * b12
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// | a15 * b12
	// | (w27, w28, w29, w30) @ (152(SP), 160(SP), 168(SP), 176(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, 160(SP)
	ADCQ $0x00, 168(SP)
	ADCQ $0x00, 176(SP)

	// |
	// | b13
	MOVQ 104(SI), CX

	// | a0 * b13
	// | (w13, w14, w15, w16) @ (40(SP), 48(SP), 56(SP), 64(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, 48(SP)
	ADCQ $0x00, 56(SP)
	ADCQ $0x00, 64(SP)

	// | a1 * b13
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a2 * b13
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a3 * b13
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a4 * b13
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a5 * b13
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a6 * b13
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a7 * b13
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a8 * b13
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a9 * b13
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a10 * b13
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a11 * b13
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a12 * b13
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a13 * b13
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// | a14 * b13
	// | (w27, w28, w29, w30) @ (152(SP), 160(SP), 168(SP), 176(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, 160(SP)
	ADCQ $0x00, 168(SP)
	ADCQ $0x00, 176(SP)

	// | a15 * b13
	// | (w28, w29, w30, w31) @ (160(SP), 168(SP), 176(SP), 184(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, 168(SP)
	ADCQ $0x00, 176(SP)
	ADCQ $0x00, 184(SP)

	// |
	// | b14
	MOVQ 112(SI), CX

	// | a0 * b14
	// | (w14, w15, w16, w17) @ (48(SP), 56(SP), 64(SP), 72(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, 56(SP)
	ADCQ $0x00, 64(SP)
	ADCQ $0x00, 72(SP)

	// | a1 * b14
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a2 * b14
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a3 * b14
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a4 * b14
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a5 * b14
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a6 * b14
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a7 * b14
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a8 * b14
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a9 * b14
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a10 * b14
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a11 * b14
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a12 * b14
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// | a13 * b14
	// | (w27, w28, w29, w30) @ (152(SP), 160(SP), 168(SP), 176(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, 160(SP)
	ADCQ $0x00, 168(SP)
	ADCQ $0x00, 176(SP)

	// | a14 * b14
	// | (w28, w29, w30, w31) @ (160(SP), 168(SP), 176(SP), 184(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, 168(SP)
	ADCQ $0x00, 176(SP)
	ADCQ $0x00, 184(SP)

	// | a15 * b14
	// | (w29, w30, w31) @ (168(SP), 176(SP), 184(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, 176(SP)
	ADCQ $0x00, 184(SP)

	// |
	// | b15
	MOVQ 120(SI), CX

	// | a0 * b15
	// | (w15, w16, w17, w18) @ (56(SP), 64(SP), 72(SP), 80(SP))
	MOVQ (DI), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, 64(SP)
	ADCQ $0x00, 72(SP)
	ADCQ $0x00, 80(SP)

	// | a1 * b15
	// | (w16, w17, w18, w19) @ (64(SP), 72(SP), 80(SP), 88(SP))
	MOVQ 8(DI), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, 72(SP)
	ADCQ $0x00, 80(SP)
	ADCQ $0x00, 88(SP)

	// | a2 * b15
	// | (w17, w18, w19, w20) @ (72(SP), 80(SP), 88(SP), 96(SP))
	MOVQ 16(DI), AX
	MULQ CX
	ADDQ AX, 72(SP)
	ADCQ DX, 80(SP)
	ADCQ $0x00, 88(SP)
	ADCQ $0x00, 96(SP)

	// | a3 * b15
	// | (w18, w19, w20, w21) @ (80(SP), 88(SP), 96(SP), 104(SP))
	MOVQ 24(DI), AX
	MULQ CX
	ADDQ AX, 80(SP)
	ADCQ DX, 88(SP)
	ADCQ $0x00, 96(SP)
	ADCQ $0x00, 104(SP)

	// | a4 * b15
	// | (w19, w20, w21, w22) @ (88(SP), 96(SP), 104(SP), 112(SP))
	MOVQ 32(DI), AX
	MULQ CX
	ADDQ AX, 88(SP)
	ADCQ DX, 96(SP)
	ADCQ $0x00, 104(SP)
	ADCQ $0x00, 112(SP)

	// | a5 * b15
	// | (w20, w21, w22, w23) @ (96(SP), 104(SP), 112(SP), 120(SP))
	MOVQ 40(DI), AX
	MULQ CX
	ADDQ AX, 96(SP)
	ADCQ DX, 104(SP)
	ADCQ $0x00, 112(SP)
	ADCQ $0x00, 120(SP)

	// | a6 * b15
	// | (w21, w22, w23, w24) @ (104(SP), 112(SP), 120(SP), 128(SP))
	MOVQ 48(DI), AX
	MULQ CX
	ADDQ AX, 104(SP)
	ADCQ DX, 112(SP)
	ADCQ $0x00, 120(SP)
	ADCQ $0x00, 128(SP)

	// | a7 * b15
	// | (w22, w23, w24, w25) @ (112(SP), 120(SP), 128(SP), 136(SP))
	MOVQ 56(DI), AX
	MULQ CX
	ADDQ AX, 112(SP)
	ADCQ DX, 120(SP)
	ADCQ $0x00, 128(SP)
	ADCQ $0x00, 136(SP)

	// | a8 * b15
	// | (w23, w24, w25, w26) @ (120(SP), 128(SP), 136(SP), 144(SP))
	MOVQ 64(DI), AX
	MULQ CX
	ADDQ AX, 120(SP)
	ADCQ DX, 128(SP)
	ADCQ $0x00, 136(SP)
	ADCQ $0x00, 144(SP)

	// | a9 * b15
	// | (w24, w25, w26, w27) @ (128(SP), 136(SP), 144(SP), 152(SP))
	MOVQ 72(DI), AX
	MULQ CX
	ADDQ AX, 128(SP)
	ADCQ DX, 136(SP)
	ADCQ $0x00, 144(SP)
	ADCQ $0x00, 152(SP)

	// | a10 * b15
	// | (w25, w26, w27, w28) @ (136(SP), 144(SP), 152(SP), 160(SP))
	MOVQ 80(DI), AX
	MULQ CX
	ADDQ AX, 136(SP)
	ADCQ DX, 144(SP)
	ADCQ $0x00, 152(SP)
	ADCQ $0x00, 160(SP)

	// | a11 * b15
	// | (w26, w27, w28, w29) @ (144(SP), 152(SP), 160(SP), 168(SP))
	MOVQ 88(DI), AX
	MULQ CX
	ADDQ AX, 144(SP)
	ADCQ DX, 152(SP)
	ADCQ $0x00, 160(SP)
	ADCQ $0x00, 168(SP)

	// | a12 * b15
	// | (w27, w28, w29, w30) @ (152(SP), 160(SP), 168(SP), 176(SP))
	MOVQ 96(DI), AX
	MULQ CX
	ADDQ AX, 152(SP)
	ADCQ DX, 160(SP)
	ADCQ $0x00, 168(SP)
	ADCQ $0x00, 176(SP)

	// | a13 * b15
	// | (w28, w29, w30, w31) @ (160(SP), 168(SP), 176(SP), 184(SP))
	MOVQ 104(DI), AX
	MULQ CX
	ADDQ AX, 160(SP)
	ADCQ DX, 168(SP)
	ADCQ $0x00, 176(SP)
	ADCQ $0x00, 184(SP)

	// | a14 * b15
	// | (w29, w30, w31) @ (168(SP), 176(SP), 184(SP))
	MOVQ 112(DI), AX
	MULQ CX
	ADDQ AX, 168(SP)
	ADCQ DX, 176(SP)
	ADCQ $0x00, 184(SP)

	// | a15 * b15
	// | (w30, w31) @ (176(SP), 184(SP))
	MOVQ 120(DI), AX
	MULQ CX
	ADDQ AX, 176(SP)
	ADCQ DX, 184(SP)

	// |
	// | Montgomerry Reduction
	MOVQ R15, 192(SP)
	MOVQ R14, 200(SP)
	MOVQ p+24(FP), R14

	// |
	// | (u @ CX) = (w0 @ R8) * inp
	MOVQ R8, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w0 @ R8
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI

	// | w1 @ R9
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w2 @ R10
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 200(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI
	ADDQ SI, 200(SP)
	ADCQ $0x00, DI

	// | w7 @ 192(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	ADDQ SI, 64(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 72(SP), R8

	// | (u @ CX) = (w1 @ R9) * inp
	MOVQ R9, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w1 @ R9
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI

	// | w2 @ R10
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w3 @ R11
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 200(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, SI
	ADDQ DI, 200(SP)
	ADCQ $0x00, SI

	// | w7 @ 192(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, DI
	ADDQ SI, 192(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	ADDQ SI, R15
	ADCQ R15, R8
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 80(SP), R9

	// | (u @ CX) = (w2 @ R10) * inp
	MOVQ R10, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w2 @ R10
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI

	// | w3 @ R11
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w4 @ R12
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 200(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI
	ADDQ SI, 200(SP)
	ADCQ $0x00, DI

	// | w7 @ 192(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w17 @ R8
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w18 @ R9
	ADDQ SI, R15
	ADCQ R15, R9
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 88(SP), R10

	// | (u @ CX) = (w3 @ R11) * inp
	MOVQ R11, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w3 @ R11
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI

	// | w4 @ R12
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w5 @ R13
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w6 @ 200(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, SI
	ADDQ DI, 200(SP)
	ADCQ $0x00, SI

	// | w7 @ 192(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, DI
	ADDQ SI, 192(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w18 @ R9
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w19 @ R10
	ADDQ SI, R15
	ADCQ R15, R10
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 96(SP), R11

	// | (u @ CX) = (w4 @ R12) * inp
	MOVQ R12, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w4 @ R12
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI

	// | w5 @ R13
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w6 @ 200(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI
	ADDQ SI, 200(SP)
	ADCQ $0x00, DI

	// | w7 @ 192(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w17 @ R8
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w18 @ R9
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w19 @ R10
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w20 @ R11
	ADDQ SI, R15
	ADCQ R15, R11
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 104(SP), R12

	// | (u @ CX) = (w5 @ R13) * inp
	MOVQ R13, AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w5 @ R13
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI

	// | w6 @ 200(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, SI
	ADDQ DI, 200(SP)
	ADCQ $0x00, SI

	// | w7 @ 192(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, DI
	ADDQ SI, 192(SP)
	ADCQ $0x00, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w18 @ R9
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w19 @ R10
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w20 @ R11
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w21 @ R12
	ADDQ SI, R15
	ADCQ R15, R12
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 112(SP), R13

	// | (u @ CX) = (w6 @ 200(SP)) * inp
	MOVQ 200(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w6 @ 200(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI

	// | w7 @ 192(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w17 @ R8
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w18 @ R9
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w19 @ R10
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w20 @ R11
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w21 @ R12
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w22 @ R13
	ADDQ SI, R15
	ADCQ R15, R13
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 120(SP), BX
	MOVQ BX, 200(SP)

	// | (u @ CX) = (w7 @ 192(SP)) * inp
	MOVQ 192(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w7 @ 192(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, DI

	// | w8 @ (SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w18 @ R9
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w19 @ R10
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w20 @ R11
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w21 @ R12
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w22 @ R13
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w23 @ 200(SP)
	ADDQ SI, R15
	ADCQ R15, 200(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 128(SP), BX
	MOVQ BX, 192(SP)

	// | (u @ CX) = (w8 @ (SP)) * inp
	MOVQ (SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w8 @ (SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI

	// | w9 @ 8(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w17 @ R8
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w18 @ R9
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w19 @ R10
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w20 @ R11
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w21 @ R12
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w22 @ R13
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w23 @ 200(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, SI
	ADDQ DI, 200(SP)
	ADCQ $0x00, SI

	// | w24 @ 192(SP)
	ADDQ SI, R15
	ADCQ R15, 192(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 136(SP), BX
	MOVQ BX, (SP)

	// | (u @ CX) = (w9 @ 8(SP)) * inp
	MOVQ 8(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w9 @ 8(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI

	// | w10 @ 16(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w18 @ R9
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w19 @ R10
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w20 @ R11
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w21 @ R12
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w22 @ R13
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w23 @ 200(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI
	ADDQ SI, 200(SP)
	ADCQ $0x00, DI

	// | w24 @ 192(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w25 @ (SP)
	ADDQ SI, R15
	ADCQ R15, (SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 144(SP), BX
	MOVQ BX, 8(SP)

	// | (u @ CX) = (w10 @ 16(SP)) * inp
	MOVQ 16(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w10 @ 16(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI

	// | w11 @ 24(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w17 @ R8
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w18 @ R9
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w19 @ R10
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w20 @ R11
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w21 @ R12
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w22 @ R13
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w23 @ 200(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, SI
	ADDQ DI, 200(SP)
	ADCQ $0x00, SI

	// | w24 @ 192(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, DI
	ADDQ SI, 192(SP)
	ADCQ $0x00, DI

	// | w25 @ (SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w26 @ 8(SP)
	ADDQ SI, R15
	ADCQ R15, 8(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 152(SP), BX
	MOVQ BX, 16(SP)

	// | (u @ CX) = (w11 @ 24(SP)) * inp
	MOVQ 24(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w11 @ 24(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI

	// | w12 @ 32(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI
	ADDQ SI, 40(SP)
	ADCQ $0x00, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w18 @ R9
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w19 @ R10
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w20 @ R11
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w21 @ R12
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w22 @ R13
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w23 @ 200(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI
	ADDQ SI, 200(SP)
	ADCQ $0x00, DI

	// | w24 @ 192(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w25 @ (SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w26 @ 8(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w27 @ 16(SP)
	ADDQ SI, R15
	ADCQ R15, 16(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 160(SP), BX
	MOVQ BX, 24(SP)

	// | (u @ CX) = (w12 @ 32(SP)) * inp
	MOVQ 32(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w12 @ 32(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI

	// | w13 @ 40(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI
	ADDQ SI, 48(SP)
	ADCQ $0x00, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w17 @ R8
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w18 @ R9
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w19 @ R10
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w20 @ R11
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w21 @ R12
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w22 @ R13
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w23 @ 200(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, SI
	ADDQ DI, 200(SP)
	ADCQ $0x00, SI

	// | w24 @ 192(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, DI
	ADDQ SI, 192(SP)
	ADCQ $0x00, DI

	// | w25 @ (SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w26 @ 8(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w27 @ 16(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w28 @ 24(SP)
	ADDQ SI, R15
	ADCQ R15, 24(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 168(SP), BX
	MOVQ BX, 32(SP)

	// | (u @ CX) = (w13 @ 40(SP)) * inp
	MOVQ 40(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w13 @ 40(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, DI

	// | w14 @ 48(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, SI
	ADDQ DI, 48(SP)
	ADCQ $0x00, SI

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI
	ADDQ SI, 56(SP)
	ADCQ $0x00, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w18 @ R9
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w19 @ R10
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w20 @ R11
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w21 @ R12
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w22 @ R13
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w23 @ 200(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI
	ADDQ SI, 200(SP)
	ADCQ $0x00, DI

	// | w24 @ 192(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w25 @ (SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w26 @ 8(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w27 @ 16(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w28 @ 24(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w29 @ 32(SP)
	ADDQ SI, R15
	ADCQ R15, 32(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 176(SP), BX
	MOVQ BX, 40(SP)

	// | (u @ CX) = (w14 @ 48(SP)) * inp
	MOVQ 48(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w14 @ 48(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 48(SP)
	ADCQ DX, DI

	// | w15 @ 56(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, SI
	ADDQ DI, 56(SP)
	ADCQ $0x00, SI

	// | w16 @ 64(SP)
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, DI
	ADDQ SI, 64(SP)
	ADCQ $0x00, DI

	// | w17 @ R8
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, SI
	ADDQ DI, R8
	ADCQ $0x00, SI

	// | w18 @ R9
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, DI
	ADDQ SI, R9
	ADCQ $0x00, DI

	// | w19 @ R10
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, SI
	ADDQ DI, R10
	ADCQ $0x00, SI

	// | w20 @ R11
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, DI
	ADDQ SI, R11
	ADCQ $0x00, DI

	// | w21 @ R12
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, SI
	ADDQ DI, R12
	ADCQ $0x00, SI

	// | w22 @ R13
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, DI
	ADDQ SI, R13
	ADCQ $0x00, DI

	// | w23 @ 200(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, SI
	ADDQ DI, 200(SP)
	ADCQ $0x00, SI

	// | w24 @ 192(SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, DI
	ADDQ SI, 192(SP)
	ADCQ $0x00, DI

	// | w25 @ (SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, SI
	ADDQ DI, (SP)
	ADCQ $0x00, SI

	// | w26 @ 8(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, DI
	ADDQ SI, 8(SP)
	ADCQ $0x00, DI

	// | w27 @ 16(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, SI
	ADDQ DI, 16(SP)
	ADCQ $0x00, SI

	// | w28 @ 24(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, DI
	ADDQ SI, 24(SP)
	ADCQ $0x00, DI

	// | w29 @ 32(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, SI
	ADDQ DI, 32(SP)
	ADCQ $0x00, SI

	// | w30 @ 40(SP)
	ADDQ SI, R15
	ADCQ R15, 40(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// |
	MOVQ 184(SP), BX
	MOVQ BX, 48(SP)

	// | (u @ CX) = (w15 @ 56(SP)) * inp
	MOVQ 56(SP), AX
	MULQ inp+32(FP)
	MOVQ AX, CX

	// | w15 @ 56(SP)
	XORQ DI, DI
	MOVQ (R14), AX
	MULQ CX
	ADDQ AX, 56(SP)
	ADCQ DX, DI

	// | w16 @ 64(SP)
	XORQ SI, SI
	MOVQ 8(R14), AX
	MULQ CX
	ADDQ AX, 64(SP)
	ADCQ DX, SI
	ADDQ DI, 64(SP)
	ADCQ $0x00, SI

	// | w17 @ R8
	XORQ DI, DI
	MOVQ 16(R14), AX
	MULQ CX
	ADDQ AX, R8
	ADCQ DX, DI
	ADDQ SI, R8
	ADCQ $0x00, DI

	// | w18 @ R9
	XORQ SI, SI
	MOVQ 24(R14), AX
	MULQ CX
	ADDQ AX, R9
	ADCQ DX, SI
	ADDQ DI, R9
	ADCQ $0x00, SI

	// | w19 @ R10
	XORQ DI, DI
	MOVQ 32(R14), AX
	MULQ CX
	ADDQ AX, R10
	ADCQ DX, DI
	ADDQ SI, R10
	ADCQ $0x00, DI

	// | w20 @ R11
	XORQ SI, SI
	MOVQ 40(R14), AX
	MULQ CX
	ADDQ AX, R11
	ADCQ DX, SI
	ADDQ DI, R11
	ADCQ $0x00, SI

	// | w21 @ R12
	XORQ DI, DI
	MOVQ 48(R14), AX
	MULQ CX
	ADDQ AX, R12
	ADCQ DX, DI
	ADDQ SI, R12
	ADCQ $0x00, DI

	// | w22 @ R13
	XORQ SI, SI
	MOVQ 56(R14), AX
	MULQ CX
	ADDQ AX, R13
	ADCQ DX, SI
	ADDQ DI, R13
	ADCQ $0x00, SI

	// | w23 @ 200(SP)
	XORQ DI, DI
	MOVQ 64(R14), AX
	MULQ CX
	ADDQ AX, 200(SP)
	ADCQ DX, DI
	ADDQ SI, 200(SP)
	ADCQ $0x00, DI

	// | w24 @ 192(SP)
	XORQ SI, SI
	MOVQ 72(R14), AX
	MULQ CX
	ADDQ AX, 192(SP)
	ADCQ DX, SI
	ADDQ DI, 192(SP)
	ADCQ $0x00, SI

	// | w25 @ (SP)
	XORQ DI, DI
	MOVQ 80(R14), AX
	MULQ CX
	ADDQ AX, (SP)
	ADCQ DX, DI
	ADDQ SI, (SP)
	ADCQ $0x00, DI

	// | w26 @ 8(SP)
	XORQ SI, SI
	MOVQ 88(R14), AX
	MULQ CX
	ADDQ AX, 8(SP)
	ADCQ DX, SI
	ADDQ DI, 8(SP)
	ADCQ $0x00, SI

	// | w27 @ 16(SP)
	XORQ DI, DI
	MOVQ 96(R14), AX
	MULQ CX
	ADDQ AX, 16(SP)
	ADCQ DX, DI
	ADDQ SI, 16(SP)
	ADCQ $0x00, DI

	// | w28 @ 24(SP)
	XORQ SI, SI
	MOVQ 104(R14), AX
	MULQ CX
	ADDQ AX, 24(SP)
	ADCQ DX, SI
	ADDQ DI, 24(SP)
	ADCQ $0x00, SI

	// | w29 @ 32(SP)
	XORQ DI, DI
	MOVQ 112(R14), AX
	MULQ CX
	ADDQ AX, 32(SP)
	ADCQ DX, DI
	ADDQ SI, 32(SP)
	ADCQ $0x00, DI

	// | w30 @ 40(SP)
	XORQ SI, SI
	MOVQ 120(R14), AX
	MULQ CX
	ADDQ AX, 40(SP)
	ADCQ DX, SI
	ADDQ DI, 40(SP)
	ADCQ $0x00, SI

	// | w31 @ 48(SP)
	ADDQ SI, R15
	ADCQ R15, 48(SP)
	MOVQ $0x0000000000000000, R15
	ADCQ $0x00, R15

	// | Reduce by modulus
	MOVQ 64(SP), CX
	SUBQ (R14), CX
	MOVQ R8, AX
	SBBQ 8(R14), AX
	MOVQ R9, DX
	SBBQ 16(R14), DX
	MOVQ R10, BX
	SBBQ 24(R14), BX
	MOVQ BX, 56(SP)
	MOVQ R11, BX
	SBBQ 32(R14), BX
	MOVQ BX, 208(SP)
	MOVQ R12, BX
	SBBQ 40(R14), BX
	MOVQ BX, 216(SP)
	MOVQ R13, BX
	SBBQ 48(R14), BX
	MOVQ BX, 224(SP)
	MOVQ 200(SP), BX
	SBBQ 56(R14), BX
	MOVQ BX, 232(SP)
	MOVQ 192(SP), BX
	SBBQ 64(R14), BX
	MOVQ BX, 240(SP)
	MOVQ (SP), BX
	SBBQ 72(R14), BX
	MOVQ BX, 248(SP)
	MOVQ 8(SP), BX
	SBBQ 80(R14), BX
	MOVQ BX, 256(SP)
	MOVQ 16(SP), BX
	SBBQ 88(R14), BX
	MOVQ BX, 264(SP)
	MOVQ 24(SP), BX
	SBBQ 96(R14), BX
	MOVQ BX, 272(SP)
	MOVQ 32(SP), BX
	SBBQ 104(R14), BX
	MOVQ BX, 280(SP)
	MOVQ 40(SP), BX
	SBBQ 112(R14), BX
	MOVQ BX, 288(SP)
	MOVQ 48(SP), BX
	SBBQ 120(R14), BX
	MOVQ BX, 296(SP)
	SBBQ $0x00, R15

	// | Compare & Return
	MOVQ    c+0(FP), DI
	CMOVQCS 64(SP), CX
	MOVQ    CX, (DI)
	CMOVQCC AX, R8
	MOVQ    R8, 8(DI)
	CMOVQCC DX, R9
	MOVQ    R9, 16(DI)
	CMOVQCC 56(SP), R10
	MOVQ    R10, 24(DI)
	CMOVQCC 208(SP), R11
	MOVQ    R11, 32(DI)
	CMOVQCC 216(SP), R12
	MOVQ    R12, 40(DI)
	CMOVQCC 224(SP), R13
	MOVQ    R13, 48(DI)
	MOVQ    200(SP), BX
	CMOVQCC 232(SP), BX
	MOVQ    BX, 56(DI)
	MOVQ    192(SP), BX
	CMOVQCC 240(SP), BX
	MOVQ    BX, 64(DI)
	MOVQ    (SP), BX
	CMOVQCC 248(SP), BX
	MOVQ    BX, 72(DI)
	MOVQ    8(SP), BX
	CMOVQCC 256(SP), BX
	MOVQ    BX, 80(DI)
	MOVQ    16(SP), BX
	CMOVQCC 264(SP), BX
	MOVQ    BX, 88(DI)
	MOVQ    24(SP), BX
	CMOVQCC 272(SP), BX
	MOVQ    BX, 96(DI)
	MOVQ    32(SP), BX
	CMOVQCC 280(SP), BX
	MOVQ    BX, 104(DI)
	MOVQ    40(SP), BX
	CMOVQCC 288(SP), BX
	MOVQ    BX, 112(DI)
	MOVQ    48(SP), BX
	CMOVQCC 296(SP), BX
	MOVQ    BX, 120(DI)
	RET



/* end 				*/

